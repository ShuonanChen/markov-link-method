% !TEX program = pdflatex
\input{preamble/preamble.tex}
\input{preamble/preamble_math.tex}
\input{preamble/preamble_acronyms.tex}

\DeclareMathOperator*{\tr}{tr}

\newcommand{\UN}[1]{\ensuremath{\left|#1\right|_\infty}}
\newcommand{\TV}[1]{\ensuremath{\left\Vert #1\right\Vert_{TV}}}
\newcommand{\EN}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\kldiv}[2]{\ensuremath{D\left(#1||#2\right)}}

\newcommand{\figgygr}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\figgygrs}[3]{\begin{tabular}{c}\includegraphics[width=#1\textwidth]{#2}\\#3\end{tabular}}
\newcommand{\figgygrsTable}[3]{\begin{tabular}{c}{#2}\\#3\end{tabular}}
\newcommand{\figgygrsH}[3]{\begin{tabular}{c}\includegraphics[width=#1\textwidth,angle=270]{#2}\\#3\end{tabular}}

% \newcommand{\figgygr}[2]{pic}
% \newcommand{\figgygrs}[3]{\begin{tabular}{c}pic\\#3\end{tabular}}
% \newcommand{\figgygrsTable}[3]{\begin{tabular}{c}{#2}\\#3\end{tabular}}
% \newcommand{\figgygrsH}[3]{\begin{tabular}{c}pic\\#3\end{tabular}}



\newcommand{\figgy}[1]{\begin{figure}\fbox{\begin{minipage}{\textwidth}#1\end{minipage}}\end{figure}}

\usepackage{cancel}

% \linenumbers

\title{The Markov Link Method: a nonparametric approach to combine observations from multiple experiments}
\author{Jackson Loper, Osnat Penn, Trygve Bakken, David Blei, Liam Paninski}

\usepackage{amsthm}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{conj}{Conjecture}

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

\begin{document}
\maketitle

\begin{abstract}
Using observations from multiple experiments is essential to building larger scientific theories.  For example, we might analyze different aspects of human neurons, such as morphology, transcriptomics, or electrophysiology.  A strong theory would account for each perspective as well as the relationship between the perspectives.  Unfortunately, we rarely have a single experiment that simultaneously measures every viewpoint, uses a sufficient sample size, and gets sufficient measurement precision.  In practice, we must therefore conduct different experiments and find a way to link the results.  The Markov Link Method (MLM) offers an algorithm to reason about these `links.' The method is nonparametric: it makes no assumptions about what the link should look like.  We evaluate the MLM on a pair of single-cell RNA techniques and gain new insight about the link between the two methods.
\end{abstract}

The last century has seen a proliferation of experimental techniques.   Different perspectives arise from different techniques (and even from different labs performing the `same' protocol).  Ideally, this yields theories that account for all perspectives.  Instead, sometimes, this leaves us with many perspectives and no idea how they are connected.  

Our goal is to connect different perspectives.  To make this idea rigorous, we assume each experiment has two aspects:
%
\begin{itemize}
    \item a \emph{sampling procedure} determines what we study
    \item a \emph{measurement procedure} defines the perspective we take in studying it.
\end{itemize}
%
If two experiments differ in both aspects, it may be difficult to gain insight by combining their data.  For example, imagine one experiment performs a transcriptomic measurement of liver cells and another applies a morphological measurement of heart cells.  Without additional experiments or knowledge, looking at these two experiments together may not be very helpful.  On the other hand, if two experiments have some aspect in common, there is something to be learned.  In this sense, a collection of experiments can be understood as a network: a pair of experiments are connected if they have some aspect in common.  In some cases, we can also connect two experiments if they differ in both aspects but we have prior knowledge (e.g.\ if the sampling procedures are different, but a reweighting can be used to correct for the difference).    When many experiments can be joined into a common network, much can be learned.  

We here propose the The Markov Link Method (MLM), an algorithm that uses a network of experiments to learn the relationship between two different perspectives.  We formalize this through what we call the `measurement link' parameters, $q^*(y|x)$.  The meaning of these parameters is defined as follows.  Let us say we obtain measurement result $x$ from one measurement procedure.  The number $q^*(y|x)$ indicates the probability of obtaining result $y$ from the second measurement procedure applied to measure the same specimen.  The link tells us how the properties measured by each procedure are associated.  For example, the link lets us answer questions such as ``I have observed the transcriptome of this cell -- what morphologies might this cell have?''

If we have a single large experiment that measures each specimen with both measurement procedures, it is straightforward to estimate the link.   We can examine specimens through both perspectives simultaneously.  However, in many cases we have some experiments in which specimens are measured with one procedure, other experiments using the other procedure, and few or no experiments where specimens are measured with both procedures.  This is particularly true if the measurement techniques destroy or alter the specimen in the process of measuring it.  

In this paper we focus particularly on the case that we have a network of experiments based on several sampling procedures and exactly two different measurement procedures.  We also suppose that the measurement procedures return categorical observations, i.e.\ a `measurement' assigns a specimen to one of a finite set of categories.  The MLM could also generalize to numerical observations, but in that case it would be appropriate to incorporate some knowledge of smoothness (e.g.\ through a Gaussian assumption).  Categorical data requires no additional modeling assumptions, so for simplicity we will here focus on the categorical case.  This setup, sketched in Figure \ref{fig:bigpicture}, is perhaps the simplest example that highlights all the important assumptions and properties of the MLM.

\figgy{
\hfill{}\figgygrs{.5}{../images/bigpicture}{(a) Setup: each specimen can be \\ measured with one of two procedures, \\ but can't be measured with both.\\ How do we study the relationship \\ between the procedures?}
\hfill{}\figgygrs{.4}{../images/plate}{(b) Key modeling assumption: \\ the link $q$ is the same for \\ every sampling procedure. }\hfill{}

\caption{{\bf The Markov Link Method (MLM) links different measurement procedures by using a set of different sampling procedures.}  In subfigure (a) we have neurons gathered using many different sampling procedures.  We select half of the neurons and determine what type of neuron they are, using transcriptomic measurements.  We classify the second half of neurons based on a morphological properties.  The MLM gives a way to link these two types of measurement and reconcile the two notions of cell-type.  In subfigure (b) we present a plate diagram articulating the statistical assumption that makes it possible (cf.\ \cite{koller2009probabilistic} for a thorough explanation of this type of diagram).  For each neuron in each group, this diagram considers the sampling procedure ($\ell$), the transcriptomic type ($X$), and the morphological type ($Y$).  The assumption is that the transcriptomic type is sufficiently detailed to \emph{statistically isolate} the transcriptomic type from the sampling procedure.  This is reflected in the arrows of the diagram; all paths from $\ell$ variables to $Y$ variables pass through $X$ variables.  The assumption allows us to estimate the link, $q$. \label{fig:bigpicture}}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%            _           
%  _ __ ___ | |_ __ ___  
% | '_ ` _ \| | '_ ` _ \ 
% | | | | | | | | | | | |
% |_| |_| |_|_|_| |_| |_|
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Markov Link Method}

\label{sec:mlm}

Imagine that we have a large collection of human cells.  For each cell we are interested in 
%
\begin{enumerate}
    \item $\ell$, the sampling procedure used to obtain the specimen.  For example, we can select different subpopulations of cells based on proteomic markers.  For each marker we might have a different sampling procedure designed to select cells with that marker.
    \item $X$, the cell's `transcriptomic type,' as measured by looking at the transcriptomic activity in the cell.  
    \item $Y$, the cell's `morphological type,' as measured by looking at an image of the cell.
\end{enumerate}
%
Our task is to reconcile the two different notions of `cell-type' indicated by $X$ and $Y$.  We would like to know what transcriptomic types are associated with what morphological types.  For example, we might like to say ``This cell has transcriptomic type 3, therefore it probably has morphological type F.''  Our main assumption is that these kinds of associations are unaffected by the choice of sampling procedure.  In particular:

\begin{center}
\fbox{\parbox[c]{4in}{
\vspace{.1in}
\begin{center}
\textbf{The Markov Link Method Assumption}\\$\mathbb{P}(Y|X,\ell)=q^*(y|x)$ for each $\ell$
\end{center}
\vspace{.1in}
}}
\end{center}

Here $\mathbb{P}(Y|X,\ell)$ indicates the probability that the morphological type is $y$ given that the transcriptomic type is $x$ and the specimen was sampled with strategy $\ell$.  The assumption is that this probability actually does not depend upon $\ell$.  This can be understood intuitively through a thought experiment.  Let us imagine we have attained perfect understanding of cellular biology.  Someone selects a cell using one of a set of sampling procedures and measures the transcriptomic activity of the cell.  We are then told the cell's transcriptomic activity (but we are not told which of the sampling procedured was used to gather the specimen).  Using our perfect knowledge of the physical systems, we could then make predictions about a measurement of the cell's morphology.  Now we are told new information: we are told which sampling procedure was used to gather the specimen.  Would this substantially change our predictions?  If so, the MLM assumption is violated.  However, if the transcriptomic measurement is sufficiently informative, learning the sampling strategy would not substantially change our predictions about the cell morphology.  As long as we can find a measurement procedure that is sufficiently informative in this way, we can apply the MLM.  A list of examples where the MLM might apply may be found in Section \ref{sec:examples}.

Our task is to estimate the link, $q^*$, from a collection of experiments.  We suppose that for each sampling procedure we have two experiments: one that measured transcriptomics and another that measured morphology.  We have no experiments in which both types were measured for the same specimens.  This setup is sketched in Figure \ref{fig:bigpicture}.  Different setups could also be considered.   For example, we might additionally have a small experiment in which both measurement procedures could be applied to the same specimen.  We might have more than two measurement procedures.  We might have measurement procedures that yield numerical observations instead of categorical ones.  Generalizing the method to all these cases should be straightforward, but we leave it for future work.  

In the specific case we will focus on, all of the experimental data can be summarized with two matrices, $N^X$ and $N^Y$.  Here $N^X_{\ell x}$ indicates the number of specimens gathered with procedure $\ell$ that have transcriptomic type $x$.  Likewise $N^Y_{\ell y}$ indicates the number of specimens gathered with procedure $\ell$ that have morphological type $y$.  For each transcriptomic type $x$ and morphological type $y$, the Markov Link Method uses the data to produce two objects:
%
\begin{enumerate}
    \item $\hat q(y|x)$ -- a point estimate for the link parameter $q^*(y|x)$
    \item $C_{x,y}$ -- a credible interval which contains the true link parameter $q^*(y|x)$ with high probability
\end{enumerate}
%
To produce these estimates for the link, we take a bayesian point of view.  It might seem sensible to start by specifying some prior beliefs about the link $q^*$.  However, the link is in many cases `nonidentifiable';  we describe this problem in detail in Appendix \ref{sec:casestudies}.  One consequence of nonidentifiability is that standard bayesian inferences would have no robustness to our precise choice of prior beliefs.  To avoid these problems, we focus on quantities which can be identified from the data.  In particular,
\begin{itemize}
    \item Let $p^*(x|\ell)$ denote the probability that a cell sampled with procedure $\ell$ will have transcriptomic type $x$.  These probabilities can be estimated from the data, $N^X$.
    \item Let $h^*(y|\ell) \triangleq \sum_x p^*(x|\ell)q^*(y|x)$ denote the probability that a cell sampled with procedure $\ell$ will have morphological type $y$.  These probabilities can be estimated from the data, $N^Y$.
    \item Let $\Theta(p,h)=\{q:\ h(y|\ell) = \sum_x p(x|\ell)q(y|x)\}$ denote the set of links which are consistent with $p,h$.  Notice that $q^*\in\Theta(p^*,h^*)$, but there may be other links $q\in\Theta(p^*,h^*)$.  Any such link yields the same distribution on what we can observe, $N^X,N^Y$.  Therefore there is simply no way to use data to distinguish among them.  This is the essence of the nonidentifiability problem mentioned above.  So, instead of trying to estimate this inestimable quantity, we will focus on three other quantities which can be identified from data.  Let
    \begin{itemize}
        \item $q_{\mathrm{lo},x,y} \triangleq \min_{q\in \Theta(p^*,h^*)} q(y|x)$
        \item $q_{\mathrm{hi},x,y} \triangleq \max_{q\in \Theta(p^*,h^*)} q(y|x)$ 
        \item $q_{\mathrm{mid}} \triangleq \argmin_{q\in \Theta(p^*,h^*)} \sum_{x,y} q(y|x)^2$
    \end{itemize}
    Note that all three of these optimization problems have unique solutions and would be straightforward to solve if $p^*,h^*$ were known; optimization details can be found in Appendix \ref{sec:optproblems}.  Morever, even when $q^*$ is nonidentifiable, we can still be assured that $q_{\mathrm{lo}}, q_{\mathrm{hi}}$ are identifiable and $q_{\mathrm{lo},x,y} \leq q^*(y|x) \leq q_{\mathrm{hi},x,y}$.  We can also hope that $q_{\mathrm{mid}}$ will provide a single estimate which strikes a middle ground; in particular, $q_{\mathrm{mid}}$ is the link value which is closest to uniform among all links which are consistent with $p^*,h^*$.  
\end{itemize}  
To summarize: it may not be possible to estimate $q^*$ from data, but we can always estimate $p^*,h^*$.  We can also compute deterministic functions of $p^*,h^*$ which bound $q^*$, namely $q_{\mathrm{lo}},q_{\mathrm{hi}}$.  We take a Bayesian point of view in estimating $p^*,h^*$.  For prior beliefs, we assume a noninformative uniform prior $\mathbb{P}$, namely
\[
\mathbb{P}(p^*,h^*) \propto 1
\]
Following the bayesian philosophy, we then incorporate new knowledge by conditioning.  We have two important pieces of knowledge we would like to incorporate.  First, we have observed the data, $N^X,N^Y$.  Second, we know from the MLM assumption that there exists \emph{some} value $q^*$ which is consistent with $p^*,h^*$.   To make the second idea formal, let $D(h|h')$ denote the Kullback-Leibler divergence and $G(p,h) = \min_{h':\ \Theta(p,h')\neq \emptyset} D(h|h')$.  $G(p,h)=0$ is a numerical way to indicate that the MLM assumption can hold.  After incorporating this knowledge, we find ourselevs interested in the conditional distribution
\[
\mathbb{P}\left(p^*,h^* | N^X,N^Y,G(p^*,h^*)=0\right)
\]
We use this posterior to produce estimates for our quantities of interest:
\begin{enumerate}
    \item $\hat q$ is calculated using posterior expectation:
    \[
    \hat q \triangleq \mathbb{E}\left[q_{\mathrm{mid}} | N^X,N^Y,G(p^*,h^*)=0\right]
    \]
    %
    \item $C_{x,y}$ is calculated using credible intervals.  For each $x,y$, we define $C_{x,y}$ to be the largest interval such that 
    \begin{align*}
    \mathbb{P}\left(q_{\mathrm{lo},x,y} \in C_{x,y} | N^X,N^Y,G(p^*,h^*)=0\right)\geq 97.5\% \\
    \mathbb{P}\left(q_{\mathrm{hi},x,y} \in C_{x,y} | N^X,N^Y,G(p^*,h^*)=0\right)\geq 97.5\% 
    \end{align*}
\end{enumerate}
Code to compute these objects is published at https://github.com/jacksonloper/markov-link-method, including a tutorial-style ipython notebook detailing every computation made in this paper.

In practice, we were not able to find a way to compute the credible interval and posterior means exactly.  It seems difficult to even sample from the conditional distribution.  Common approaches to this type of problem involve Markov-Chain Monte Carlo and Variational Inference, but we were unable to make these approaches work in practice.  It seems nontrivial to work with the condition $G(p,h)=0$ that formalizes the MLM assumption.  We instead take a somewhat na\"ive approach.  We start by defining $\tilde P,\tilde H$ to carry the conditional distribution of $p^*,h^*$ given the observed data (but not conditioned on the fact that the MLM assumption holds).  We then simply project $\tilde H$ to the space of parameters which are consistent with the MLM assumption.  That is, we take $\hat H$ to be the solution of the minimization problem $\min_{h'}D(\tilde H|h')$, subject to the constraint that $E(\tilde P,h')=0$.  Optimization details can be found in Appendix \ref{sec:optproblems}.  We hope that the distribution of $\tilde P,\hat H$ will approximate the desired posterior.  We then use samples from this distribution to compute what we need.  We asymptotically expect that $\tilde P,\tilde H$ will nearly satisfy the MLM assumption in any case, so this approximation should not make a large difference.  However, we caution that the exact implications of the approximation are difficult to reason about.   In any case, it is a practical solution to a difficult problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                 _           
%   _____  ____ _ _ __ ___  _ __ | | ___  ___ 
%  / _ \ \/ / _` | '_ ` _ \| '_ \| |/ _ \/ __|
% |  __/>  < (_| | | | | | | |_) | |  __/\__ \
%  \___/_/\_\__,_|_| |_| |_| .__/|_|\___||___/
%                          |_|               
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Examples where the Markov Link Method may apply}

\label{sec:examples}

The validity of the Markov Link Method assumption for a given situation should be closely contemplated.  Let us consider a few real-world examples where this assumption may apply.

\begin{itemize}
    \item Quality control for manufacturing.   One way to test the reliability of a part is to construct a machine that pushes the part until it breaks.  However, how can we test the reliability of the machine that performs the test?  In each test run there will be some variability induced by the machine itself, which induces a measurement error.  In practice, some kind of assumptions about part homogeneity are used to approximate this error (cf.\ \cite{de2005gauge}).  However, if we have two testing machines we can use the MLM to obtain a calibration between the machines, even though we can never test the same part with both machines.  This enables us to bound the overall measurement error.  In this case, $\ell$ might indicate the type of a part being tested, $X$ would indicate the reliability of a part as measured by one machine, and $Y$ would indicate the reliability of a part as measured by another machine.  If the error in machine $Y$ is not correlated to the part type $\ell$, then the MLM assumption certainly holds.  Even if the error is correlated, the MLM assumption may still hold.  For example, imagine that the $Y$ error is correlated with the absolute reliability of the part; this may pose no problem if that reliability is adequately measured by $X$.  

    \item Radiometric calibration.  Different cameras measure light differently.  For example, each camera has different lens distortions.  Different cameras also have different ways of transforming photon counts into digital information.   Fortunately, joint measurement is generally possible with cameras; simply take a picture of the same object with both cameras.  Unfortunately, an adequate amount of joint measurement is sometimes hard to come by.  For example, with expensive astronomy-grade settings, it can be difficult to balance the need for calibration with the total amount of the sky one wants to cover (cf.\ \cite{padmanabhan2008improved}). For example, instead of requiring different cameras to take pictures of exactly the same portion of sky at exactly the same time, the subpopulations $\ell$ could represent portions of the sky.  Various conditions may cause these portions of the sky to appear differently over time, but if we assume this variability is independent of the calibration itself, the MLM assumption may apply.  
 
    \item Cancer treatment efficacy prediction.  Starting from in-vivo human cancers, many cell-lines have been cultured over the years.  These cell cultures live indefinitely on plates.  Many experiments have been performed to see how these cancer cells respond to treatment.  However, if a treatment works on a particular cultured cell-line, what can we say about whether a treatment will work on an actual in-vivo cancer inside a patient?  Coarse side-information such as original cancer location is often available for both in-vivo and cultured cells, but this is often a surprisingly weak signal.  Cell transcriptomes provides much more specific information about the cancer, and thus, in theory, what treatments might be appropriate (cf.\ \cite{cieslik2018cancer}).  However, we know that cultured cell-lines look quite different from in-vivo cells (cf. \cite{imamura2015comparison,haibe2013inconsistency}).   These cell cultures are subject to quite different pressures, due to the fact that they survive on a plate instead of inside a human being.  The Markov Link method can leverage the common side-information together with separate transcriptome information to understand the correspondence between in-vivo and cultured cells.  If a particular drug is effective on a particular cultured cell-line, we can then look at the corresponding in-vivo transcriptomic profile.  If we find human cancers that match this profile, they might be good candidates for further research using this particular drug.  Here $\ell$ might indicate cancer location, $X$ might indicate transcriptomic expression of cultured cells, and $Y$ might indicate transcriptomic expression of in-vivo cells.  As the transcriptomic expression is much more informative than the cancer location, it is plausible that $X$ might be sufficient to explain any correlations between $\ell$ and $Y$.  Thus the MLM assumption may hold.

    \item Text/image correspondence.  Automatic image captioning is an ongoing effort in machine learning (cf.\ \cite{srivastava2018survey}).  There are three types of data available to help develop such algorithms: text-only data, image-only data, and paired-text-and-image data.  Obviously the last kind is the most useful for automatic image captioning, but there is much less of it.  The Markov Link Method suggests one way to use the more plentiful text-only and image-only data.  We can first apply classic machine learning techniques to get coarse labels for both kinds of data.  Using this side-information to identify subpopulations, the MLM can then deduce a fine-grained correspondence between text and images by combining information from across all the subpopulations.  Here $\ell$ would indicate coarse labels such as ``cat'' or ``street scene.''  These labels could be derived from either images or text and can be trained in a supervised fashion.  $X$ would indicate the image and $Y$ would indicate a caption.  If the caption is largely determined by the picture $X$, the MLM assumption may hold.  

    \item Replication crisis and lab effects.  Replicating a published study is not always an easy thing to do.  This difficulty is commonly attributed to selective publication bias, bad design, poor description of methods, and even outright fraud \cite{baker2016reproducibility}.  However, some of the problem may simply be a matter of calibration.  If two labs perform identical experiments and get different data, that does not mean we need to throw out both datasets.  Instead, we can use MLM to calibrate the processes used by each lab.  Once the labs are properly calibrated, we can meaningfully combine both datasets.  Unlike other tools to deal with lab or batch effects (e.g. \cite{crow2018characterizing,johnson2007adjusting}), MLM makes zero assumptions about what calibrations we might expect.  In this case, $\ell$ would indicate subpopulations which both labs could access.  For example, we can take several batches of mice; for each batch we can send half to one lab and half to the other lab.  $X$ will the indicate the full results from each specimen examined in one lab and $Y$ coarser information from specimens examined in the other.  If the $X$ data is sufficiently detailed, the MLM assumption may hold.  
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%      _                 _       _   _                 
%  ___(_)_ __ ___  _   _| | __ _| |_(_) ___  _ __  ___ 
% / __| | '_ ` _ \| | | | |/ _` | __| |/ _ \| '_ \/ __|
% \__ \ | | | | | | |_| | | (_| | |_| | (_) | | | \__ \
% |___/_|_| |_| |_|\__,_|_|\__,_|\__|_|\___/|_| |_|___/
%                                                    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Mathematical Results and Simulations}

\label{sec:simulations}


\figgy{
\hfill{}\figgygrs{.4}{../images/simulationI}{(a) Estimation error, \\ varying number of samples \\ and number of sampling procedures.}\hfill{}
\hfill{}\figgygrs{.5}{../images/simulationIconf}{(b) Credible intervals, \\ varying number of samples \\ and number of sampling procedures.}\hfill{}

\hfill{}\figgygrs{.7}{../images/simulationII}{(c) Estimation error, varying number \\ of samples and strength of link dependency.}\hfill{}


\caption{{\bf When can the MLM accurately estimate the link?} \label{fig:simI}  We use simulations to test the MLM.  In subfigure (a) we look at the error of the MLM link estimator in many different situations, varying the numbers of samples, sampling procedures, and the link parameters.  In each trial our `ground truth' link parameters picked uniformly at random from the parameter space.  The MLM estimator accurately recovers the ground truth from simulated data, as long as we have enough samples \emph{and} enough distinct sampling procedures.  In subfigure (b) we consider the MLM's credible intervals.  For a selection of trials we choose a random value of of $x,y$ and then compare the true link parameter $q^*(y|x)$ with the MLM interval $C_{x,y}$.  We show the interval (in black) centered around the ground truth (in red).  Since the link parameters lie in $[0,1]$, the interval is always contained within $\pm1$ of the ground truth (this larger region is shown in gray).  The intervals mostly contain the truth, and get narrower with more samples as long as there are enough distinct sampling procedures.   In subfigure (c) we focus on the case of exactly $4$ sampling procedures.  In this case it is only possible to get very accurate estimates if the link takes on a particular form.  If the link is independent (i.e.\ $X,Y$ are independent) or if the link is invertible and deterministic (e.g.\ $X=Y$) then we can determine what the link is.  However, if the truth is more complex, small numbers of sampling procedures may make it impossible to determine the true link; this issue is discussed in detail in Section \ref{sec:casestudies}.}
}

There are three desiderata we might hope the Markov Link Method estimators would achieve:
\begin{description}
    \item[Estimator convergence] As we obtain more samples, $\hat q(y|x)$ should converge to the true link $q^*(y|x)$ for each $x,y$.
    \item[Interval concentration] As we obtain more samples, the interval $C_{x,y}$ should get smaller. Asymptotically it should include nothing but the point $q^*(y|x)$.
    \item[Conservative coverage] $q^*(y|x) \in C_{x,y}$ with high probability.
\end{description}
However, due to certain identifiability issues, it is only actually possible to guarantee the last desideratum.  In many cases, the MLM achieves all three.  However, we can show that identifiability issues block any possibility of success in some cases; we detail this problem in Appendix \ref{sec:casestudies}.  The good news is that even though the Markov Link Method credible intervals may not concentrate around the truth, they do as well as possible.  In particular, they do appear to converge around the quantities we can estimate, namely the $q_\mathrm{lo},q_\mathrm{hi}$ bounds described in Section \ref{sec:mlm}.  A precise  statement of what we have been able to prove may be found in Theorem \ref{thm:mainthm} of Appendix \ref{sec:casestudies}.

The following two simulations give a good overview of the relevant issues:

\begin{itemize}
    \item How does MLM performance depend on the number of samples and the number of sampling procedures?  We suppose we have two measurement procedures which return categorical measurements among six categories.  Thus the first procedure yields $X\in \{1,2,3,4,5,6\}$ and the second yields $Y\in \{1,2,3,4,5,6\}$.  To see how the method performs in different circumstances, we will run many trials.  In each trial we fix the number of sampling procedures and pick `ground truth' parameters uniformly at random from the parameter space.  We then simulate a dataset from these ground truth parameters, fixing the total number of samples (spreading these samples equally among all combinations of sampling procedure and measurement procedure).  Finally, we apply the MLM to the simulated data to get the point estimates $\tilde q(y|x)$ and the credible intervals $C_{x y}$.  We measure the overall estimator convergence using a kind of total variation distance:
    \[
    \mathrm{Error}(\tilde q,q^*)=\frac{1}{12}\sum_{x=1}^{6}\sum_{y=1}^{6} |\tilde q(y|x) - q^*(y|x)|
    \]
    This error ranges between zero and one.  Zero error indicates that $\tilde q=q^*$. An error of one indicates that the estimate has completely incorrect beliefs about the probability mass, i.e. $q^*(y|x)=0$ whenever $\tilde q(y|x)>0$ and vice-versa.  We also look at the MLM credible intervals and see how often they cover the true parameters.  

    The results are summarized in Figure \ref{fig:simI}.  In trials with more samples, the estimator generally has lower error.  However, to make the error actually converge to zero we need at least six distinct sampling procedures.  This figure also shows that the intervals work correctly regardless of of the number of samples or sampling procedures; they include the ground truth with high probability.   With many samples and sampling procedures, the intervals are small and concentrated around the truth.  With fewer samples or sampling procedures, the intervals are sometimes forced to be larger.  However, our uncertainty is not the same for every parameter.  In some cases we are able to obtain a very tight credible interval for one particular parameter even though the overall estimator error is high.  

    \item How does estimator convergence depend upon the link parameters themselves?  In each trial of this simulation we use four sampling procedures and our measurement procedures yield one of six categories.  In the previous simulations we saw that estimator convergence was generally impossible in this situation, due to the small number of sampling procedures.  However, in our previous simulations we picked the link parameters uniformly from parameter space.  Now we will be more choosey.   On one extreme, we will produce trials where the link makes $X,Y$ independent, i.e. $q^*(y|x)=q^*(y|x')$ for every $x,x'$.  On the other extreme, we will have trials where $X,Y$ are deterministically related by the equation $X=Y$.  We will also consider every link ``in-between'' these two extremes (found by convex combinations).  Figure \ref{fig:simI} shows that estimator convergence is actually possible in the two extreme cases.  However, estimator convergence fails for the in-between cases.  We leave a complete mathematical understanding of this result to future work.  For the purposes of this paper, we content ourselves by noting one interesting case: let the measurements return one of $2^k$ categories and let us use only $k+1$ carefully chosen sampling procedures.  Now suppose the link defines any invertible deterministic function between $X$ and $Y$.  In this case, with enough enough samples, we can determine both that the relationship is deterministic and the exact specification of the invertible function.  This result is proven in Appendix \ref{sec:casestudies}, Theorem \ref{thm:miracle}.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                 _     _       _        
%  _ __ ___  __ _| | __| | __ _| |_ __ _ 
% | '__/ _ \/ _` | |/ _` |/ _` | __/ _` |
% | | |  __/ (_| | | (_| | (_| | || (_| |
% |_|  \___|\__,_|_|\__,_|\__,_|\__\__,_|
                                       
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Empirical results for cell-types}

\figgy{
\hfill{}\figgygr{.95}{../images/allenNlxy}\hfill{}
\caption{{\bf The input to the Markov Link Method: one experiment for each measurement procedure and each sampling procedure.}  Here we show a portion of the real-world data to which we applied the MLM.  The Allen Institute gathered neurons from the visual cortex of mice, using a variety of cre/lox-based sampling procedure (cf.\ \cite{tasic2017shared}).  Each strategy was designed to bring out different subpopulations of cells.  Neurons were measured to determine their `type,' using one of two procedures: Standard or Patch.  Standard recognizes 104 different types of neurons.  Patch has a coarser notion of cell-type, distinguishing only 10 types.  For each experiment, we tabulated the number of cells assigned to each type.  Above we show a subset of these results through heatmaps; the color of each square indicates the number of specimens found to have a particular type.  Using this kind of data, our task is to calibrate the two classification protocols.  That is, we want to be able to ask question of the following form: ``if a neuron is classified as being of type `Peri Kcnj8' by Standard, how might it have been classified by Patch?'' \label{fig:alleninput}}
}

\subsection{Background}

Our motivation for this problem arose from the Allen Institute's investigation of neuronal cell-types.  The institute has a variety of methods for examining a neuron and determining what kind of neuron it is.  However, many of these procedures destroy the neurons in the process of measuring them.  We'll look at two procedures in particular, which we'll call `Standard' and 'Patch.'  Standard uses a single-cell RNA sequencing pipeline to determine transcriptomic expression in the cell and determines the cell-type based on these results.  Patch also uses transcriptomic information from RNA sequencing.  Patch additionally obtains electrophysiological and morphological properties of the neuron.  This comes at the cost of a degraded transcriptomic signal, requiring new methods to estimate the cell-type.  We refer the reader to \citep{tasic2017shared} for details on the two methods.

Best efforts were made to use biological intuition to calibrate the methods.  For example, Patch has a notion of a ``Lamp5'' cell type.  Standard gives a more granular analysis, dividing this type into many sub-types, such as ``Lamp5 Pdlim5'' and ``Lamp5 Slc35d3.''  If a cell was designated as ``Lamp5 Pdlim5'' using Standard, the hope was that it be given the ``Lamp5'' type by Patch.  The two methods were designed to achieve this goal.  However, each method has its own biases and errors, and it was not obvious whether this effort was successful.  In particular, it seemed clear that sometimes a cell labelled one way with one method would get labelled quite differently with another method, but it was not clear how often this occurred.  

Fortunately, there was a kind of information that seemed like it might help determine whether the two methods were properly calibrated: a variety of sampling procedures.  Using a cre/lox system (cf.\ \citep{tasic2017shared}) they were able to pick out various overlapping subpopulations of neurons using different procedures.  Each procedure was expected to yield different proportions of the different cell-types.  For each sampling procedure and each measurement procedure, many specimens were sampled and their cell-types determined.  The result of this process was two tables, parts of which are shown in Figure \ref{fig:alleninput}.  While it seemed clear that these tables should say something about the calibration, it was not obvious how to best use this information.  It was for this purpose that the MLM was developed.

\subsection{Results}

\figgy{
\hfill{}\figgygr{0.9}{../images/allenout}\hfill{}
\caption{{\bf MLM estimation of the link, with credible interval.}  The central plot shows a portion of the MLM estimator applied to the Allen Institute data.  We look at a subselection of Standard types ($x$) and all of the Patch types ($y$).  For each combination $x,y$ we draw a rectangle whose color indicates the value of the estimator $\hat q(y|x)$.  We also show our confidence about these estimators.  On the left we indicate the lower bounds indicated by the credible interval, on the right we indicate upper bounds.  For some parameters our intervals are much tighter than others.  For example, it appears we have high confidence the Standard type `Vip Rspo4' is highly associated with the Patch type `Vip.'  By constrast, we have almost no idea what is associated with the Standard type `Meis2.'  The upper credible interval bounds suggest that $q^*(y|\mathrm{Meis2})$ could be nearly 1 for many different Patch types.  Obviously it cannot be 1 for all of those types simultaneously, since $\sum_y q^*(y|\mathrm{Meis2})=1$, but the data simply doesn't tell us which $y$ carries the mass.  \label{fig:allenboot}}
}

We applied the MLM to this data to obtain point estimates $\tilde q(y|x)$ and credible intervals $C_{x y}$.  In Figure \ref{fig:allenboot} we visualize these objects for selected values of $x,y$.  Each parameter has a slightly different story.  For example, for the Standard type $x=$`Vip Rspo4' and the Patch type $y=$`Vip,' we have that $C_{x,y}=[.92,1.0]$.  This supports the idea that the true link satisfies $q^*(y|x)\geq .92$: at least 92\% of the cells classified as type `Vip Rspo4' by the Standard method will be classified as `Vip' by the Patch method.  However, for other types there is more ambiguity.  For the Standard type $x=$`Lamp5 Egln 1' and the Patch types $y=$`Lamp' we have $C_{x y}=[0,1.0]$.  The data we have does not give us a definitive answer as to whether cells with Standard type `Lamp5 Egln 1' are being classified with Patch type `Lamp.'

The variability in the credible regions suggests what we need to do in order to more closely determine the value of the calibration.  For example, if we could develop more unique sampling strategies which will include `Lamp5 Egln 1' cells, this would help us resolve our ambiguity about this aspect of the link.  Indeed, going back to the original data, it is easy to see why this ambiguity appeared in the first place.  Cells of the `Lamp5 Egln 1' type only appear in any number when using the sampling strategies `Gad2-IRES-Cre' and 'Slc32a1-IRES-Cre.'  Both of those sampling strategies yield a fairly similar mix of types when measured with Patch.  To get a better resolution of the calibration, we would need a sampling strategy that included `Lamp5 Egln 1' cells but represents a significantly different slice of the overall population.  For a particular proposed experiment, simulations such as those found in Section \ref{sec:simulations} can be used to determine how many samples might be required to get an accurate estimate of the link, $q^*$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%            _                               _    
%  _ __  _ __(_) ___  _ ____      _____  _ __| | __
% | '_ \| '__| |/ _ \| '__\ \ /\ / / _ \| '__| |/ /
% | |_) | |  | | (_) | |   \ V  V / (_) | |  |   < 
% | .__/|_|  |_|\___/|_|    \_/\_/ \___/|_|  |_|\_\
% |_|                                              
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Relation to prior work}

The task of this paper was infer the link between different measurement procedures, so that experimental data can be meaningfully combined.  There is an enormous literature on this subject.  For example, when experiments are performed in batches, the exact measurement procedures can vary slightly between batches.  The entire field of `batch effects' is devoted to handling these problems.  The general approach is to use some knowledge of the procedures to make modeling assumptions about the links.  These assumptions give us a way to estimate the link (cf.\ \cite{johnson2007adjusting}).  If different measurement procedures yield results in the same space, we can also implicitly articulate these kinds of assumptions by placing a metric on the space.  We suppose that different measurement procedures should yield results that are `close' in this metric.  We can then use optimal transport techniques to produce a link based on these assumptions (cf.\ \cite{tabak2018explanation}).  From the most general point of view, we are engaged in meta-analysis; we refer the reader to \cite{borenstein2011introduction} for a general introduction to the field.  

The main distinguishing characteristics of this paper are two-fold: we place no assumptions on the nature of the link and we take the identifiability issues seriously.  We refer the reader to \cite{walter2014identifiability} for an introduction to what we mean by identifiability.  We discuss the identifiability issues for our problem in particular in Appendix \ref{sec:casestudies}.  Our only assumption is that the sampling procedures can be statistically isolated from the link.  This assumption says nothing about what the link itself is.  We take a Bayesian approach, but we only use our prior beliefs to estimate quantities which actually can be estimated, carefully avoiding the identifiability issues.  We use these quantities to produce intervals which account for uncertainty due both to small sample sizes and to identifiability issues.

The main technical contribution of this paper was figuring out how to use the MLM assumption to get credible intervals that worked in practice.  In this we were inspired by a large literature of examples where assumptions are used to bound potentially unidentifiable parameters.  Some of this literature comes from the field of causality.  For example, in \cite{bonet2001instrumentality} Bonet produces regions not unlike the ones seen here to explore whether a variable can be used as an instrument.  The famous Clauser-Horne-Shimony-Holt inequality was designed to help answer causality questions in quantum physics, but it also sheds light on what distributions are consistent with certain assumptions \cite{clauser1969proposed}.  More generally, the physics literature has contributed many key assumptions that bound unidentifiable parameters (cf. \cite{chaves2014inferring}, \cite{kela2017semidefinite}, and the references therein).  Perhaps the closest work to this one would be \cite{makarov1982estimates}, which used two marginal distributions to get bounds on a property of the joint distribution (namely the distribution of the sum).  We advance this approach to a more general-purpose technique, both by using many subpopulations to closely refine our estimates and by considering the entire space of possible joint distributions instead of simply a particular property of the joint.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                       _           _                 
%   ___ ___  _ __   ___| |_   _ ___(_) ___  _ __  ___ 
%  / __/ _ \| '_ \ / __| | | | / __| |/ _ \| '_ \/ __|
% | (_| (_) | | | | (__| | |_| \__ \ | (_) | | | \__ \
%  \___\___/|_| |_|\___|_|\__,_|___/_|\___/|_| |_|___/
%                                                    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Discussion}

It can be difficult to understand how two different measurement procedures are related to each other.  This makes it difficult to understand how to combine knowledge from experiments which use different procedures.  The problem is particularly tricky if we cannot look at the same specimen through the perpsective of both procedures simultaneously.

In this paper we suggest one way to overcome these difficulties.  We formalize the `relationship' between two measurement procedures through the notion of a `measurement link.'  Given that we have measured a specimen with one procedure and obtained the outcome $x$, the link parameter $q(y|x)$ tells us the probability that we would obtain outcome $y$ if we measured the same specimen with the second procedure.  We present the Markov Link Method assumption, which roughly states that the link can be statistically isolated from the sampling procedures.  We provide the Markov Link Method (MLM) algorithm that uses this assumption to estimate the link and measure our uncertainty.  Code is published at https://github.com/jacksonloper/markov-link-method, including a tutorial-style ipython notebook detailing every computation made in this paper.

The MLM appears gives provable guarantees, provides reasonable values in simulation, and gives useful insight to real data.  Applied to neuronal cell-type data, the MLM clarified how two different cell-type classification systems are related.  We saw that some aspects of the two systems seem well-calibrated, but others we are less sure about.  The nature of the variability in the credible intervals suggested directions for experiments which would further refine our understanding.  

In future work, it would be interesting to use additional assumptions to help us estimate calibrations.  For example, as it stands the Markov Link Method will only yield narrow credible intervals if each measurement tool returns one of a finite number of measurement outcomes.  In some cases, we may believe that similar measurement values should have similar probabilities.  Such smoothness assumptions would make it possible to apply the MLM to measurement tools which can return a continuum of values.   

Another direction for future work would be to take a frequentist point of view.  An obvious direction would be to use profile likelihoods as statistics to define confidence intervals for the link parameters.  However, the distribution of these statistic is difficult to pin down.  Traditional techniques based on asymptotic normality may fail dramatically, because the true parameters may lie at the boundary of the parameter space.  In particular, there may be some $x,y$ for which $q^*(y|x)=0$.  Even approximate methods are difficult to apply because the parameter space can be quite high-dimensional.  Perhaps future work could uncover a way to overcome these challenges.

It is clear that good assumptions can help us get real insight for tough problems. Even if these assumptions are not sufficient to allow us to perfectly identify our object of interest, we can bound our uncertainty.  By probing this uncertainty carefully, we can learn what the data actually has to say and what experiments will help us learn more.

\bibliographystyle{unsrt}
\bibliography{refs}

\appendix


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                   __ _     _                     
%   ___ ___  _ __  / _(_) __| | ___ _ __   ___ ___ 
%  / __/ _ \| '_ \| |_| |/ _` |/ _ \ '_ \ / __/ _ \
% | (_| (_) | | | |  _| | (_| |  __/ | | | (_|  __/
%  \___\___/|_| |_|_| |_|\__,_|\___|_| |_|\___\___|
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 

\section{Numerical issues}

\label{sec:optproblems}

There are three numerical problems which the MLM must solve.  Here we detail our method for solving each of them.

\begin{enumerate}

    \item Projecting to the MLM assumption.  Fix any parameters $P,\tilde H$.  One step in the MLM involves projecting $\tilde H$ to the set of parameters which are consistent with $P$ and the MLM assumption.  In particular, we defined
    \[
    D(h|h') = \sum_{\ell,y} h(y|\ell) \log \frac{h(y|\ell)}{h'(y|\ell)}
    \]
    and we needed to solve the problem
    \[
    \min_{h} D(\tilde H|h)
    \]
    subject to the constraint that there exists some $q$ such that $h(y|\ell)=\sum_x p(x|\ell)q(y|x)$.  Parametrizing valid $h$ through $q$, we obtain the problem
    \[
    \max_{q} \sum_{\ell,y} \tilde H(y|\ell) \log \left(\sum_x P(x|\ell) q(y|x)\right)
    \]
    Taking derivatives one can readily show that this problem is convex.  We solve it using an iterative method.  We initially guess that $q$ is uniform.  We then repeatedly make the updates
    \begin{align*}
    q(y|x)  &\propto q(y|x)\sum_{\ell}p(x|\ell)\frac{\tilde H(y|\ell)}{\sum_x p(x|\ell)q(y|x)}\\
    \end{align*}
    until convergence.  Our convergence criteria is that all parameters change less than $10^{-5}$ in a single iteration.

    \item Linear programming.  Fix any $P,H$.  To deal with the identifiability issues, we defined $\Theta(p,h)=\{q:\ h(y|\ell) \triangleq \sum_x p(y|\ell)q(y|x)\}$.  The MLM requires us to solve linear optimization problems within $\Theta$, such as 
    \[
    \min_{q\in \Theta(P,H)} q(y|x) \\
    \]
    We solve these problems using the {\tt cvxopt} python package.

    \item Quadratic programming.  The MLM also requires us to solve quadratic optimization problems within $\Theta$:
    \[
    \min_{q\in \Theta(P,H)} \sum q(y|x)^2 \\
    \]
    We solve these problems using the {\tt cvxopt} python package.


\end{enumerate}

\section{Identifiability}

\label{sec:casestudies}

The issue of identifiability comes up repeatedly throughout this paper.  Here we give a brief overview of the fundamentals of this issue and how it effects us.  We also present two suggestive case studies which we hope may inspire future research.  In both cases we are able to prove something of interest -- but not quite as much as we might hope.  

Consider experiments yielding an $\Omega_\ell\times\Omega_X$ matrix $N^X$ and an $\Omega_\ell \times \Omega_Y$ matrix $N^Y$, sampled according to
\begin{align*}
(N^X_{\ell 1},N^X_{\ell 2}\cdots N^X_{\ell \Omega_X}) \sim \mathrm{Multinomial}(n_\ell,p^*(\cdot | \ell))\\
(N^Y_{\ell 1},N^Y_{\ell 2}\cdots N^Y_{\ell \Omega_Y}) \sim \mathrm{Multinomial}(m_\ell,h^*(\cdot | \ell))
\end{align*}
where $p^*(x|\ell)$ is some conditional distribution and there is some $q^*(y|x)$ such that $h^*(y|\ell)=\sum_x p^*(x|\ell)q^*(y|x)$.  This is simply a restatement of the assumptions we have made throughout the data about how our parameters of interest $(p^*,q^*,h^*)$ are related to the data we can observe $(N^X,N^Y)$.

It is well-known that we can obtain arbitrarily good estimates of $p^*,h^*$ by taking enough samples (i.e.\ taking $n_\ell,m_\ell$ sufficiently high).  Let us therefore imagine for a moment that we in fact have \emph{perfect knowledge} of $p^*,h^*$.  Even so, the data does not necessarily tell us the value of the link parameters $q^*$.  There may be many possible links, $q$, which are all equally consistent with $p^*,h^*$.  Each such link yields the exact same distribution on the data we can observe, so there can be no way to use data to distinguish among them.  This is known as a `nonidentifiability problem.'  Even with infinite data, we simply cannot identify exactly what the value of $q^*$ might be.

We will now look at some examples:

\subsection{A simple failure case}

Consider the case that $\Omega_\ell = \Omega_Y = 2$ and $\Omega_X=3$.  That is, there are $2$ separate sampling procedures, tool I recognizes 3 categories and tool II recognizes 2 categories.  In particular, let us imagine that $p^*(x|\ell)=A_{\ell x}$ and $h^*(y|\ell)=B_{\ell y}$ where $A,B$ are matrices given by 
%
\begin{align*}
A&=\left(\begin{array}{ccc}
\frac{1}{2} & \frac{1}{2} & 0\\
\frac{2}{6} & \frac{1}{6} & \frac{3}{2}
\end{array}\right) \\
B&=\left(\begin{array}{cc}
\frac{1}{2} & \frac{1}{2}\\
\frac{2}{3} & \frac{1}{3}
\end{array}\right)
\end{align*}

Rows correspond to different sampling procedures and columns correspond to a different measurement outcome.  Now let $q^*(y|x)=C_{x y}$, another matrix.  The Markov Link Method assumption then tells us that $A\times C=B$, where $\times$ indicates matrix multiplication.  This corresponds to $\Omega_\ell \times \Omega_Y = 4$ equations.  We also have a normalizing constraint that $\sum y q^*(y|x)=1$, which creates $\Omega_X=3$ additional equations.  However, these normalizing constraints actually make two of the MLM assumption constraints redundant.  In the end, we have $5$ constraining equations on the matrix $C$.  However, the matrix $C$ contains six numbers.  The result is a degree of freedom in $C$, corresponding to an aspect of $q^*$ that we simply cannot resolve.  For example, here are two choices of $C$ which are both consistent with the equation $A\times C=B$:
\[
C=\left(\begin{array}{cc}
0 & 1\\
1 & 0\\
1 & 0
\end{array}\right)\qquad C=\left(\begin{array}{cc}
1 & 0\\
0 & 1\\
\frac{2}{3} & \frac{1}{3}
\end{array}\right)
\]



\subsection{Permutation matrices}

Consider the case that $\Omega_\ell = k$ and $\Omega_X=\Omega_Y=2^{k-1}$.  That is, there are $k$ separate subpopulations, and both tool I and tool II can return one of $2^{k-1}$ possible values.  Let us furthermore assume that
\[
q^*(y|x)=\begin{cases}1 \quad \mathrm{if}\ x=y\\0\quad \mathrm{else}\end{cases}
\]
and  $p^*(x|\ell)=A_{\ell,x}$, where this matrix is given by
\[
A=2^{2-k}\left(\begin{array}{ccccccccccc}
0 & 1 & 0 & 1 & 0 & 1 & \cdots & 0 & 1 & 0 & 1\\
0 & 0 & 1 & 1 & 0 & 0 & \cdots & 0 & 0 & 1 & 1\\
0 & 0 & 0 & 0 & 1 & 1 & \cdots & 1 & 1 & 1 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 1 & 1 & 1\\
 &  &  &  &  &  & \vdots\\
0 & 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 1 & 1 & 1\\
1 & 0 & 1 & 0 & 1 & 0 & \cdots & 1 & 0 & 1 & 0
\end{array}\right)
\]
That is, the $x$th column of the first $k-1$ rows is the binary expansion of the number $x-1$, and the last row alternates $1$s and $0$s.  Now let us say we have perfect knowledge of $p^*(x|\ell)$ and $h^*(y|\ell)=\sum_x p^*(y|\ell)q^*(y|x)$.  Notice that due to the simple structure of $q^*$ we obtain $h^*(y|\ell)=A^{\ell,y}$.  However, let us imagine we know nothing about the true value of $q^*$.  

How much can we say about $q^*$, if we only had knowledge of $p^*$ and $h^*$?  On the one hand, we observe that in the absence of any other constraints, the object $q^*$ has $2^{2k-3}$ degrees of freedom.  This is because there are $2^{k-1}$ values of $\ell$ and for each subpopulation $q^*(\cdot|\ell)$ must lie in the $2^{k-2}$-dimensional simplex on $2^{k-1}$ atoms.  On the other hand, we see that the Markov Link Assumption gives us $k\times(2^{k-1}-1)$ linear constraints on the value of $q$.  Indeed, for each subpopulation in $1\cdots k$ and each value of $y \in 1 \cdots 2^{k-1}$ we have an equation of the form
\[
\sum_x p(y|\ell)q(y|x)=h(y|\ell)
\]
Of these $k\times2^{k-1}$ constraints, $k$ of them are redundant with the fact that $\sum_y q(y|x)=1$.  Thus, altogether, the Markov Link Assumption together with approximate knowledge of $p,h$ gives us $k\times(2^{k-1}-1)$ linear constraints.  It would follow that $q$ would have $2^{2k-3} - k\times(2^{k-1}-1)$ degrees of freedom yet remaining.  

In conclusion, a simple degrees-of-freedom counting argument would suggest that there will be substantial ambiguity about what value $q$ might take on, if our only knowledge about $q$ is that it must satisfy $\sum_x p^*(y|\ell)q(y|x)=h^*(y|\ell)$.  Indeed, we have \emph{exponentially many} more degrees of freedom than we have constraints.  

However, the reality is that $q$ is exactly determined by $p^*,h^*$.  This is possible because there are inequality constraints which also govern $q$, namely $q(y|x)\geq 0$.  Thus, while a simple degrees-of-freedom counting argument might suggest that we would have substantial identifiability issues in this problem, the reality is quite the opposite.  This idea is made rigorous in the following theorem.  

\begin{thm}\label{thm:miracle}
Let $p^*,h^*$ be as they are defined above.  Then there is exactly one $q$ that is consistent with $p^*,h^*$ and the Markov Link assumption.  That is, $q^*$ is the only possible $q$ satisfying
\begin{gather*}
\sum_y q(y|x)=1 \\
\sum_x A_{\ell,x}q(y|x)=A_{\ell,y}\\
q(y|x)\geq 0
\end{gather*}
\end{thm} 
\begin{proof}
We prove by recursion.  First take the case $k=2$.  In this case the result holds trivially, since $X,Y\in\{1\}$.

Now consider a general case $k>2$.  Let us focus on the constraints implied by the second-to-last row population.  It is straightforward to see that these constraints imply
\begin{align*}
0=&q(y|x) \qquad \forall y\leq 2^{k-2},x>2^{k-2}
\end{align*}
Indeed, for each $y\leq 2^{k-2}$ we obtain a constraint showing that $\sum_{x>2^{k-2}}q(y|x)=0$, which yields that in fact $q(y|x)=0$ for every $x>2^{k-2}$ and every $y\leq 2^{k-2}$.

It follows that for $y\leq 2^{k-2}$ our original constraints may be rewritten as
\[
\sum_{x\leq2^{k-2}} A_{\ell x} q(x|y) = A_{\ell y} \qquad \forall y\leq 2^{k-2}
\]
This is an example of the same problem we started with -- except with $k$ one smaller.  Applying our inductive hypothesis, we may thus obtain that $q(y|x)=q^*(y|x)$ for the first $2^{k-2}$ values of $x,y$.  Moreover, since $\sum_{y\leq 2^{k-2}} q^*(y|x)=1$, we see that $q$ must also satisfy $q(y|x)=0$ for $y>2^{k-2}$ and $x\leq 2^{k-2}$.  Thus we have seen that $q=q^*$ for all entries except those in which $x,y\geq 2^{k-2}$.

For $x,y \geq2^{k-2}$ we linearly combine equations concerning the first, last, and second to last rows of $A$ with factors of $1,1,-1$ respectively.  We obtain constraints showing that $\sum_{x\leq2^{k-2}}q(y|x)=0$ for each $y>2^{k-2}$.  We can then use the same reasoning to obtain that $q=q^*$ for the remaining values of $x,y$.
\end{proof}

This result is somewhat robust to slight perturbations in $p^*,h^*$.  In particular, if we have some $\hat p\approx p^*$ and $\hat h\approx h^*$ then at each stage of the argument we can replace statements of the form $q(y|x)=0$ with statements of the form $q(y|x)\leq \epsilon$.  Applying this with the kinds of arguments above will show that we can be sure that $\hat q$ is arbitrarily close to $q^*$ if we know that $\hat p,\hat h$ are sufficiently close to $p^*,h^*$.  

However, it turns out that the relationship between $q$ and $p^*,h^*$ is not robust in every situation.  In the next section we will see that it can in fact be quite discontinuous:

\subsection{Discontinuity}

Consider the case that $\Omega_\ell = 1$ and $\Omega_X=\Omega_Y=2$.  That is, there is only one sampling procedure (no subpopulations) and both tool I and tool II can return one of $2$ possible values.  We will now consider two possiblities:
\begin{enumerate}
\item First let us take the case
    \begin{itemize}
    \item $\mathbb{P}(X=1)=p^*(1)=0$
    \item $\mathbb{P}(X=2)=p^*(2)=1$
    \item $\mathbb{P}(Y=1)=h^*(1)=0$
    \item $\mathbb{P}(Y=2)=h^*(2)=1$
    \end{itemize}
    In this case the MLM assumption $\sum_x p^*(x)q(y|x)=h^*(y)$ can be used to prove that $q(1|2)=0,q(2|2)=1$, but we now have \emph{absolutely no} knowledge of $q(1|1),q(2|1)$.  This is because we simply never observed the case $X=1$ (it occurs with probability zero), and so we cannot possibly have any knowledge about $q(y|x)$ for $x=1$.  
\item Now let us take a slight variation:
    \begin{itemize}
    \item $\mathbb{P}(X=1)=p^*(1)=0.01$
    \item $\mathbb{P}(X=2)=p^*(2)=0.99$
    \item $\mathbb{P}(Y=1)=h^*(1)=0$
    \item $\mathbb{P}(Y=2)=h^*(2)=1$
    \end{itemize}
    In this case we can again prove that $q(1|2)=0,q(2|2)=1$, but we can also prove that $q(1|1)=0,q(2|1)=1$.  
\item Now we take yet another slight variation:
    \begin{itemize}
    \item $\mathbb{P}(X=1)=p^*(1)=0.01$
    \item $\mathbb{P}(X=2)=p^*(2)=0.99$
    \item $\mathbb{P}(Y=1)=h^*(1)=0.01$
    \item $\mathbb{P}(Y=2)=h^*(2)=0.99$
    \end{itemize}
    In this case we can prove that $q(1|2)\leq 1/99$ and $q(2|1)\geq 1-1/99$, but we again cannot prove almost anything about $q(2|1)$.  In particular, it is easy to produce cases in which $q(2|1)=0$ and other cases in which $q(2|1)=1$.  

\end{enumerate}

The disturbing thing about this example is that by making infinitesimal perturbations to $p^*$ we can pass from uncertainty to complete certainty back to uncertainty.  It is for this reason that in this paper we refuse to ever treat $\hat p,\hat h$ as fixed and given, always considering the space of perturbations around any such values.  

It is worth noting that these kinds of problems essentially vanish if the true $q^*$ is bounded away from zero i.e.\ $q^*(y|x)>c$ for every $x,y$ for some $c>0$.  If this holds, together with a certain linear independence assumption, we can guarantee that the true $q^*$ is close to the set $\Theta(\hat p,\hat h)$ when $\hat p,\hat h$ are good approximations to $p^*,h^*$, where
\[
\Theta(p,h) \triangleq \left\{q\in T:\ \sum_x p(x|\ell)q(y|x)= h(y|\ell) \quad \forall \ell,y \right\}
\]
and by $T$ we mean the transition matrix polytope, $T=\{q:\ q(y|x)\geq 0, \sum_y q(y|x)=1\}$.  In particular:\vspace{.1in}

\begin{thm}\label{thm:mainthm}
Fix any $q^*$ satisfying $q^*(y|x)>c$ for some $c>0$.  Let us further assume that the matrix $B^*_{\ell,x}=p^*(x|\ell)$ has linearly independent rows.  Fix any $p^*$ and define $h^*(y|\ell)=\sum_x p^*(x|\ell)q^*(y|x)$.  Then by taking any $\hat p,\hat h$ sufficiently close to $p^*,h^*$ we can ensure that $q^*$ is arbitrarily close to some point in the set $\hat \Theta = \Theta(\hat p,\hat h)$.
\end{thm}
\begin{proof}
Let $\hat A$ denote the affine plane $\hat A=\{q:\ \sum_x \hat p(x|\ell)q(y|x)= \hat  h(y|\ell)\}$.  Thus $\hat \Theta = T \cap \hat A$.

Now fix any $\hat p,\hat h$.  Now let $\tilde q$ be the orthogonal Euclidean projection of $q^*$ to the affine space $A$.  That is, $\tilde q$ is minimizes a sum-of-squares difference to $q^*$ among all the points in $A$.  The linearly independent rows of $B^*$ allow us to bound the spectral norm of the right-pseudoinverse of $\hat B_{\ell,x}=\hat p(x|\ell)$, by taking $\hat p$ sufficiently close to $p^*$.  If we furthermore require $\hat h$ sufficiently close to $h^*$, we can use this to ensure the projection distance is small.  That is, we can force $\tilde q$ to be arbitrarily close to $q^*$.  Using the fact that $q^*(y|x)>c$ we can thereby furthermore insure that $\tilde q(y|x)\geq0$.  Finally, it is straightforward to see that the projection leaves the constraint $\sum_y q(y|x)=1$ unchanged.  Thus $\tilde q\in \hat \Theta$ and $\tilde q$ is arbitrarily close to $q^*$. 
\end{proof}

Since it is easy to find consistent estimators for $\hat p,\hat h$, this theorem suggests we can use those estimators to get good estimates for our uncertainty about $q^*$.  In particular, subject to the conditions of the theorem, we have that the bounds of $C_{x,y}$ converge to the bounds of what is identifiable about $q^*$.  It is certainly a step in the right direction, but we emphasize that the theorem's conditions are nontrivial.  It is our opinion that the linear independence condition is fairly mild (if it is not met then subpopulations can simply be merged together).  However, the positivity condition is quite troubling.  In many cases of interest the true link $q^*$ has genuine zeros: pairs of measurements between the two tools which are fundamentally incompatible.  In this case such a theorem cannot be applied.  

However, it may be that the above theorem's requirement ($q(x|y)>c>0$) is much stronger than is necessary.  A precise understanding of the discontinuity example above has remained elusive.  Better understanding could lead to more accurate estimates.  We leave it for future work.


\end{document}






































