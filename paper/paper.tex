% !TEX program = pdflatex
\input{preamble/preamble.tex}
\input{preamble/preamble_math.tex}
\input{preamble/preamble_acronyms.tex}

\DeclareMathOperator*{\tr}{tr}

\newcommand{\plx}{\ensuremath{f}}
\newcommand{\pxy}{\ensuremath{g}}
\newcommand{\ply}{\ensuremath{h}}
% \newcommand{\DX}{\ensuremath{{\mathcal{D}^X}}}
% \newcommand{\DY}{\ensuremath{{\mathcal{D}^Y}}}
\newcommand{\DX}{\ensuremath{{\mathcal{D}_X}}}
\newcommand{\DY}{\ensuremath{{\mathcal{D}_Y}}}
% \newcommand{\DX}{\ensuremath{\mathcal{D}^X}}
% \newcommand{\DY}{\ensuremath{\mathcal{D}^Y}}
\newcommand{\MLMH}{\ensuremath{\mathcal{A}}}

\newcommand{\MLM}{MLM}


\newcommand{\UN}[1]{\ensuremath{\left|#1\right|_\infty}}
\newcommand{\TV}[1]{\ensuremath{\left\Vert #1\right\Vert_{TV}}}
\newcommand{\EN}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\kldiv}[2]{\ensuremath{D\left(#1||#2\right)}}

\newcommand{\figgygr}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\figgygrs}[3]{\begin{tabular}{c}\includegraphics[width=#1\textwidth]{#2}\\#3\end{tabular}}
\newcommand{\figgygrsTable}[3]{\begin{tabular}{c}{#2}\\#3\end{tabular}}
\newcommand{\figgygrsH}[3]{\begin{tabular}{c}\includegraphics[width=#1\textwidth,angle=270]{#2}\\#3\end{tabular}}

% \newcommand{\figgygr}[2]{pic}
% \newcommand{\figgygrs}[3]{\begin{tabular}{c}pic\\#3\end{tabular}}
% \newcommand{\figgygrsTable}[3]{\begin{tabular}{c}{#2}\\#3\end{tabular}}
% \newcommand{\figgygrsH}[3]{\begin{tabular}{c}pic\\#3\end{tabular}}



\newcommand{\figgy}[1]{\begin{figure}\fbox{\begin{minipage}{\textwidth}#1\end{minipage}}\end{figure}}

\usepackage{cancel}

% \linenumbers

\title{The Markov link method: a nonparametric approach to combine observations from multiple experiments}
\author{Jackson Loper, Osnat Penn, Trygve Bakken, David Blei, Liam Paninski, \\Additional Authors To Be Determined}

\usepackage{amsthm}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{conj}{Conjecture}

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

\begin{document}
\maketitle

\begin{abstract}
The Markov link method (\MLM) synthesizes experiments based on different measurement procedures.  For example, we might have different experiments measuring different aspects of human neurons, such as morphology, transcriptomics, or electrophysiology.  To combine knowledge across these experiments, we develop the idea of a ``link'' between measurement procedures.  This link should answer questions such as: ``We have observed the transcriptome of this cell -- what might its morphology have been?'' Without joint measurement of the transcriptome and morphology for the same neurons, it is challenging to estimate the link.  However, it turns out the problem is not impossible.  The \MLM{} estimates these links without joint measurement and without any assumptions about what the link looks like. Instead, the \MLM{} relies on a conditional independence assumption.  The estimation problem arising from this assumption is challenging, due to nonidentifiability problems; we give a rigorous characterization of the issue and show that the \MLM{} handles it correctly.  We prove conditions under which the \MLM{} estimators are consistent, and simulations show that it gives accurate measures of uncertainty.  We evaluate the \MLM{} on a pair of single-cell RNA techniques and gain new insight about the link between the two methods; we find that the two different notions of `cell-type' are generally well-aligned, with a few exceptions.  The \MLM{} also shows that some aspects of the link simply cannot be determined from the data we have.  The \MLM{} suggests specific additional experiments which would help refine our understanding of the relationship between the two measurement procedures.
\end{abstract}

It is sometimes difficult to understand how to synthesize experiments which use different measurement procedures.  We assume each experiment has two aspects:
%
\begin{itemize}
    \item A \emph{selection procedure} defines how we select cells.  For example, one experiment might select only liver cells, and another might select only cells which express the Vip protein.  
    \item A \emph{measurement procedure} defines what we measure about each cell.  For example, one experiment might measure the transcriptomic profile of a cell, and another experiment might measure the cell morphology.
\end{itemize}
%
To synthesize experiments with different measurement procedures, we must understand how the measurement procedures are related.  For example, we would like to be able to answer questions such as ``We have measured the transcriptome of this cell -- what might its morphology have been?''  We formalize this idea through a conditional probability distribution we call the `measurement link,' $\pxy(y|x)$.  Specifically, let us say we obtain measurement result $x$ from one measurement procedure.  The number $\pxy(y|x)$ indicates the probability of obtaining result $y$ from the second measurement procedure applied to measure the same specimen.  

We here propose the The Markov link method (\MLM), an algorithm that uses a collection of experiments to estimate the measurement link, \pxy.  In this paper we focus particularly on the case that we have experiments based on several selection procedures and exactly two different measurement procedures.  We also suppose that the measurement procedures return categorical observations, i.e.\ a `measurement' assigns a specimen to one of a finite set of categories.  The \MLM{} could also generalize to numerical observations, but in that case it would be appropriate to incorporate some knowledge of smoothness (e.g.\ through a Gaussian assumption).  Categorical data requires no additional modeling assumptions, so for simplicity we will here focus on the categorical case.  This setup, sketched in Figure \ref{fig:bigpicture}, is perhaps the simplest example that highlights all the important assumptions and properties of the \MLM.  It embodies a common situation in biological problems, such as is found in \cite{gouwens2018classification}. 

\figgy{
\hfill{}\figgygrs{.5}{../images/bigpicture}{(a) Setup: each specimen can be \\ measured with one of two procedures, \\ but can't be measured with both.\\ How do we study the relationship \\ between the procedures?}
\hfill{}\figgygrs{.4}{../images/plate}{(b) Key modeling assumption: \\ the link $\pxy$ is the same for \\ every selection procedure. }\hfill{}

\caption{{\bf The Markov link method (MLM) links different measurement procedures by using a set of different selection procedures.}  In subfigure (a) we have neurons gathered using many different selection procedures.  We select half of the neurons and determine what type of neuron they are, using transcriptomic measurements.  We classify the second half of neurons based on a morphological properties.  The MLM gives a way to link these two types of measurement and reconcile the two notions of cell-type.  In subfigure (b) we present a plate diagram articulating the statistical assumption that makes it possible (cf.\ \cite{koller2009probabilistic} for a thorough explanation of this type of diagram).  For each neuron in each group, this diagram considers the selection procedure ($\ell$), the transcriptomic type ($X$), and the morphological type ($Y$).  The assumption is that the transcriptomic type is sufficiently detailed to \emph{statistically isolate} the transcriptomic type from the selection procedure.  This is reflected in the arrows of the diagram; all paths from $\ell$ variables to $Y$ variables pass through $X$ variables.  The assumption allows us to estimate the link, $\pxy$. \label{fig:bigpicture}}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%            _           
%  _ __ ___ | |_ __ ___  
% | '_ ` _ \| | '_ ` _ \ 
% | | | | | | | | | | | |
% |_| |_| |_|_|_| |_| |_|
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Markov link method}

\label{sec:mlm}

The Markov link method is a general way to estimate measurement links using a collection of experiments.  For concreteness, we will use experiments about human cells as a running example.  Let us say that we have gathered many human cells.  For each cell we are interested in 
%
\begin{enumerate}
    \item $\ell$, the selection procedure used to obtain the cell.  For example, $\ell=1$ might correspond to a procedure that selects only cells exhibiting one proteomic marker, whereas $\ell=2$ might select cells with a different marker.
    \item $X$, the cell's `transcriptomic type,' as measured through a transcriptomic analysis.  
    \item $Y$, the cell's `morphological type,' as measured by looking at an image of the cell.
\end{enumerate}
%
The \MLM{} gives us a way to link the transcriptomic and morphological perspectives on `cell-type,' as manifested through the $X$ and $Y$ measurement procedures.  For example, we might like to say ``This cell has transcriptomic type 3, therefore it probably has morphological type F.''  

The \emph{Markov link method assumption} states that these kinds of associations are unaffected by the choice of selection procedure.  That is, $\mathbb{P}(Y|X,\ell)=\pxy(y|x)$ for each $\ell$, where $\mathbb{P}(Y|X,\ell)$ indicates the probability that the morphological type is $y$ given that the transcriptomic type is $x$ and the specimen was sampled with strategy $\ell$.  The assumption states that that this probability actually does not depend upon $\ell$.  The graphical model in figure \ref{fig:bigpicture} embodies precisely this assumption.  Intuitively, we can understand the assumption using a thought experiment.  Let us imagine we have attained perfect understanding of cellular biology.  Someone selects a cell using one of a set of selection procedures and measures the transcriptomic activity of the cell.  We are then told the cell's transcriptomic type (but we are not told which of the selection procedures was used to gather the specimen).  Using our perfect knowledge of the physical systems, we could then make predictions about the cell's morphological type.  Now we are told new information: we are told which selection procedure was used to gather the specimen.  Would this substantially change our predictions?  If so, the MLM assumption is violated.  However, if the transcriptomic measurement is sufficiently informative, learning the sampling strategy would not substantially change our predictions about the cell morphology.  As long as we can find a measurement procedure that is sufficiently informative in this way, we can apply the MLM.  A list of examples where the MLM might apply may be found in Section \ref{sec:examples}.

The purpose of the \MLM{} is to use data and the \MLM{} assumption to estimate the link $\pxy$ from a collection of experiments.  We suppose that for each selection procedure we have two experiments: one that measured transcriptomics and another that measured morphology.  We have no experiments in which both types were measured for the same specimens.  This setup is sketched in Figure \ref{fig:bigpicture}.  Different setups could also be considered.   For example, we might additionally have a small experiment in which both measurement procedures could be applied to the same specimen.  We might have more than two measurement procedures.  We might have measurement procedures that yield numerical observations instead of categorical ones.  Generalizing the method to all these cases should be straightforward, but we leave it for future work.  

In the specific case we will focus on, all of the experimental data can be summarized with two matrices, $\DX$ and $\DY$.  The matrix $\DX$ tabulates the transcriptomic types found in specimens gathered with each selection procedure; $\DY$ does the same for morphological types.   For each transcriptomic type $x$ and morphological type $y$, the Markov link method uses this data to produce two objects:
%
\begin{enumerate}
    \item $\hat \pxy(y|x)$ -- a point estimate for the link $\pxy(y|x)$
    \item $C_{x,y}$ -- a credible interval which contains the true link $\pxy(y|x)$ with high probability
\end{enumerate}
%
To estimate these, we must consider certain `nuisance' quantities.  The link is the primary target of study, but we cannot estimate the link without considering these other objects.  The observable data, $\DX,\DY$, is governed by the following conditional distributions:
\begin{itemize}
    \item $\plx(x|\ell)$ -- the probability that a cell sampled with procedure $\ell$ will have transcriptomic type $x$
    \item $\ply(y|\ell)$ -- the probability that a cell sampled with procedure $\ell$ will have morphological type $y$
\end{itemize}
It is straightforward to estimate $\plx,\ply$ from $\DX,\DY$ using standard methods.  We would like to use these distributions to tell us something about the the link, $\pxy$.  Under the Markov link method assumption, this link is connected to $\plx,\ply$ through the equations
\begin{gather}\label{eq:mlmh}
\sum_x \plx(x|\ell)\pxy(y|x)=\ply(y|\ell) \qquad \forall y,\ell
\end{gather}
With these equations and knowledge of $\plx,\ply$, we can produce an estimate for the link: $\hat \pxy_{\plx,\ply}$.  If the equations are underdetermined, we can also produce bounds for the link.  We define these objects precisely in Appendix \ref{sec:mlmdetails}.  However, in practice we do not actually have exact knowledge of $\plx,\ply$.  We only have estimates from data.  We take a Bayesian perspective to account for uncertainty in this estimation.  For prior beliefs, we assume a noninformative uniform prior $\mathbb{P}$.  Following the Bayesian philosophy, we then incorporate new knowledge by conditioning.  We have two important pieces of knowledge about $\plx,\ply$.  First, we have observed the data, $\DX,\DY$.  Second, we know from the MLM assumption that there exists \emph{some} value $\pxy$ such that Equation (\ref{eq:mlmh}) holds; let \MLMH{} denote the event that this holds.  By conditioning on this knowledge, we obtain the posterior distribution $\mathbb{P}(\plx,\ply | \DX,\DY,\MLMH)$.  The \MLM{} point estimate $\hat \pxy$ is then defined as a Bayes estimator for $\hat \pxy_{\plx,\ply}$ with respect to this posterior:
\[
\hat \pxy(y|x) \triangleq \mathbb{E}\left[\hat \pxy_{\plx,\ply} | \DX,\DY,\MLMH \right]
               = \int \hat \pxy_{\plx,\ply} \mathbb{P}(\plx,\ply | \DX,\DY,\MLMH) d\plx,d\ply
\]
This estimate minimizes the expected squared risk error under the posterior distribution.  We can use similar strategies to produce the credible intervals $C_{x,y}$.  Exact details can be found in Appendix \ref{sec:mlmdetails}.\footnote{Code to compute $\hat \pxy(y|x)$ and $C_{x,y}$ is published at https://github.com/jacksonloper/markov-link-method, including a tutorial-style ipython notebook detailing every computation made in this paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                 _           
%   _____  ____ _ _ __ ___  _ __ | | ___  ___ 
%  / _ \ \/ / _` | '_ ` _ \| '_ \| |/ _ \/ __|
% |  __/>  < (_| | | | | | | |_) | |  __/\__ \
%  \___/_/\_\__,_|_| |_| |_| .__/|_|\___||___/
%                          |_|               
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Examples}

\label{sec:examples}

The \MLM{} is a general approach for synthesizing experiments, which requires that the Markov link method assumption holds.  Here are some examples where the \MLM{} could be applied:

\begin{itemize}
    \item Quality control for manufacturing.   One way to test the reliability of a part is to construct a machine that pushes the part until it breaks.  However, how can we test the reliability of the machine that performs the test?  In each test run there will be some variability induced by the machine itself, which induces a measurement error.  In practice, some kind of assumptions about part homogeneity are used to approximate this error (cf.\ \cite{de2005gauge}).  However, if we have two testing machines we can use the MLM to obtain a calibration between the machines, even though we can never test the same part with both machines.  This enables us to bound the overall measurement error.  In this case, $\ell$ might indicate the type of a part being tested, $X$ would indicate the reliability of a part as measured by one machine, and $Y$ would indicate the reliability of a part as measured by another machine.  If the error in machine $Y$ is not correlated to the part type $\ell$, then the MLM assumption certainly holds.  Even if the error is correlated, the MLM assumption may still hold.  For example, imagine that the $Y$ error is correlated with the absolute reliability of the part; this may pose no problem if that reliability is adequately measured by $X$.  

    \item Radiometric calibration.  Different cameras measure light differently.  For example, each camera has different lens distortions.  Different cameras also have different ways of transforming photon counts into digital information.   Fortunately, joint measurement is generally possible with cameras; simply take a picture of the same object with both cameras.  Unfortunately, an adequate amount of joint measurement is sometimes hard to come by.  For example, with expensive astronomy-grade settings, it can be difficult to balance the need for calibration with the total amount of the sky one wants to cover (cf.\ \cite{padmanabhan2008improved}). For example, instead of requiring different cameras to take pictures of exactly the same portion of sky at exactly the same time, the subpopulations $\ell$ could represent portions of the sky.  Various conditions may cause these portions of the sky to appear differently over time, but if we assume this variability is independent of the calibration itself, the MLM assumption may apply.  
 
    \item Cancer treatment efficacy prediction.  Starting from in-vivo human cancers, many cell-lines have been cultured over the years.  These cell cultures live indefinitely on plates.  Many experiments have been performed to see how these cancer cells respond to treatment.  However, if a treatment works on a particular cultured cell-line, what can we say about whether a treatment will work on an actual in-vivo cancer inside a patient?  Coarse side-information such as original cancer location is often available for both in-vivo and cultured cells, but this is often a surprisingly weak signal.  Cell transcriptomes provides much more specific information about the cancer, and thus, in theory, what treatments might be appropriate (cf.\ \cite{cieslik2018cancer}).  However, we know that cultured cell-lines look quite different from in-vivo cells (cf. \cite{imamura2015comparison,haibe2013inconsistency}).   These cell cultures are subject to quite different pressures, due to the fact that they survive on a plate instead of inside a human being.  The Markov link method can leverage the common side-information together with separate transcriptome information to understand the correspondence between in-vivo and cultured cells.  If a particular drug is effective on a particular cultured cell-line, we can then look at the corresponding in-vivo transcriptomic profile.  If we find human cancers that match this profile, they might be good candidates for further research using this particular drug.  Here $\ell$ might indicate cancer location, $X$ might indicate transcriptomic expression of cultured cells, and $Y$ might indicate transcriptomic expression of in-vivo cells.  As the transcriptomic expression is much more informative than the cancer location, it is plausible that $X$ might be sufficient to explain any correlations between $\ell$ and $Y$.  Thus the MLM assumption may hold.

    \item Text/image correspondence.  Automatic image captioning is an ongoing effort in machine learning (cf.\ \cite{srivastava2018survey}).  There are three types of data available to help develop such algorithms: text-only data, image-only data, and paired-text-and-image data.  Obviously the last kind is the most useful for automatic image captioning, but there is much less of it.  The Markov link method suggests one way to use the more plentiful text-only and image-only data.  We can first apply classic machine learning techniques to get coarse labels for both kinds of data.  Using this side-information to identify subpopulations, the MLM can then deduce a fine-grained correspondence between text and images by combining information from across all the subpopulations.  Here $\ell$ would indicate coarse labels such as ``cat'' or ``street scene.''  These labels could be derived from either images or text and can be trained in a supervised fashion.  $X$ would indicate the image and $Y$ would indicate a caption.  If the caption is largely determined by the picture $X$, the MLM assumption may hold.  

    \item Replication crisis and lab effects.  Replicating a published study is not always an easy thing to do.  This difficulty is commonly attributed to selective publication bias, bad design, poor description of methods, and even outright fraud \cite{baker2016reproducibility}.  However, some of the problem may simply be a matter of calibration.  If two labs perform identical experiments and get different data, that does not mean we need to throw out both datasets.  Instead, we can use MLM to calibrate the processes used by each lab.  Once the labs are properly calibrated, we can meaningfully combine both datasets.  Unlike other tools to deal with lab or batch effects (e.g. \cite{crow2018characterizing,johnson2007adjusting}), MLM makes zero assumptions about what calibrations we might expect.  In this case, $\ell$ would indicate subpopulations which both labs could access.  For example, we can take several batches of mice; for each batch we can send half to one lab and half to the other lab.  $X$ will the indicate the full results from each specimen examined in one lab and $Y$ coarser information from specimens examined in the other.  If the $X$ data is sufficiently detailed, the MLM assumption may hold.  
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%      _                 _       _   _                 
%  ___(_)_ __ ___  _   _| | __ _| |_(_) ___  _ __  ___ 
% / __| | '_ ` _ \| | | | |/ _` | __| |/ _ \| '_ \/ __|
% \__ \ | | | | | | |_| | | (_| | |_| | (_) | | | \__ \
% |___/_|_| |_| |_|\__,_|_|\__,_|\__|_|\___/|_| |_|___/
%                                                    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Mathematical Results and Simulations}

\label{sec:simulations}


\figgy{
\hfill{}\figgygrs{.4}{../images/simulationI}{(a) Estimation error, \\ varying number of samples \\ and number of selection procedures.}\hfill{}
\hfill{}\figgygrs{.5}{../images/simulationIconf}{(b) Credible intervals, \\ varying number of samples \\ and number of selection procedures.}\hfill{}

\hfill{}\figgygrs{.7}{../images/simulationII}{(c) Estimation error, varying number \\ of samples and strength of link dependency.}\hfill{}


\caption{{\bf When can the MLM accurately estimate the link?} \label{fig:simI}  We use simulations to test the MLM.  In subfigure (a) we look at the error of the MLM link estimator in many different situations, varying the numbers of samples, selection procedures, and the link itself.  In each trial the `ground truth' link is picked uniformly at random from the parameter space.  The MLM estimator accurately recovers the ground truth from simulated data, as long as we have enough samples \emph{and} enough distinct selection procedures.  In subfigure (b) we consider the MLM's credible intervals.  For a selection of trials we choose a random value of of $x,y$ and then compare the true link $\pxy(y|x)$ with the MLM interval $C_{x,y}$.  We show the interval (in black) centered around the ground truth (in red).  Since the link probabilities lie in $[0,1]$, the interval is always contained within $\pm1$ of the ground truth (this larger region is shown in gray).  The intervals mostly contain the truth, and get narrower with more samples as long as there are enough distinct selection procedures.   In subfigure (c) we focus on the case of exactly $4$ selection procedures.  In this case it is only possible to get very accurate estimates if the link takes on a particular form.  If the link is independent (i.e.\ $X,Y$ are independent) or if the link is invertible and deterministic (e.g.\ $X=Y$) then we can determine what the link is.  However, if the truth is more complex, small numbers of selection procedures may make it impossible to determine the true link; this issue is discussed in detail in Section \ref{sec:casestudies}.}
}

There are three desiderata we might hope the Markov link method estimators would achieve:
\begin{description}
    \item[Estimator convergence] As we obtain more samples, $\hat \pxy(y|x)$ should converge to the true link $\pxy(y|x)$ for each $x,y$.
    \item[Interval concentration] As we obtain more samples, the interval $C_{x,y}$ should get smaller. Asymptotically it should include nothing but the point $\pxy(y|x)$.
    \item[Conservative coverage] $\pxy(y|x) \in C_{x,y}$ with high probability.
\end{description}
In many cases, the MLM achieves all three of these conditions.  In other cases, we can show that an issue known as `identifiability' blocks even the possibility of estimator convergence or interval concentration.  However, even when interval concentration is impossible, the \MLM{} appears to do as well as it possibly can: the intervals $C_{x,y}$ close around the bounds of what can possibly be learned from the data we observe.  We have begun to develop a theory of the relevant issues, which we detail in Appendix \ref{sec:mlmdetails}.  We give a few of the interesting corner-cases and prove a few surprising results.  A complete story remains elusive.  The following two simulations give a good overview of the relevant issues:

\begin{itemize}
    \item How does MLM performance depend on the number of samples and the number of selection procedures?  We suppose we have two measurement procedures which return categorical measurements among six categories.  Thus the first procedure yields $X\in \{1,2,3,4,5,6\}$ and the second yields $Y\in \{1,2,3,4,5,6\}$.  To see how the method performs in different circumstances, we will run many trials.  In each trial we fix the number of selection procedures and pick a `ground truth' link uniformly at random from the parameter space.  We then simulate a dataset from this ground truth link, fixing the total number of samples (spreading these samples equally among all combinations of selection procedure and measurement procedure).  Finally, we apply the MLM to the simulated data to get the point estimates $\tilde q(y|x)$ and the credible intervals $C_{x y}$.  We measure the overall estimator convergence using a kind of total variation distance:
    \[
    \mathrm{Error}(\tilde q,\pxy)=\frac{1}{12}\sum_{x=1}^{6}\sum_{y=1}^{6} |\tilde q(y|x) - \pxy(y|x)|
    \]
    This error ranges between zero and one.  Zero error indicates that $\tilde q=\pxy$. An error of one indicates that the estimate has completely incorrect beliefs about the probability mass, i.e. $\pxy(y|x)=0$ whenever $\tilde q(y|x)>0$ and vice-versa.  We also look at the MLM credible intervals and see how often they cover the true parameters.  

    The results are summarized in Figure \ref{fig:simI}.  In trials with more samples, the estimator generally has lower error.  However, to make the error actually converge to zero we need at least six distinct selection procedures.  This figure also shows that the intervals work correctly regardless of of the number of samples or selection procedures; they include the ground truth with high probability.   With many samples and selection procedures, the intervals are small and concentrated around the truth.  With fewer samples or selection procedures, the intervals are sometimes forced to be larger.  However, the uncertainty is not the same for every aspect of the link.  In some cases we are able to obtain a very tight credible interval for one particular value even though the overall estimator error is high.  

    \item How does estimator convergence depend upon the link itself?  In each trial of this simulation we use four selection procedures and the measurement procedures yield one of six categories.  In the previous simulations we saw that estimator convergence was generally impossible in this situation, due to the small number of selection procedures.  However, in previous simulations we picked the link uniformly from its parameter space.  Now we will be more choosey.   On one extreme, we will produce trials where the link makes $X,Y$ independent, i.e. $\pxy(y|x)=\pxy(y|x')$ for every $x,x'$.  On the other extreme, we will have trials where $X,Y$ are deterministically related by the equation $X=Y$.  We will also consider every link ``in-between'' these two extremes (found by convex combinations).  Figure \ref{fig:simI} shows that estimator convergence is actually possible in the two extreme cases.  However, estimator convergence fails for the in-between cases.  We leave a complete mathematical understanding of this result to future work.  For the purposes of this paper, we content ourselves by noting one interesting case: let the measurements return one of $2^k$ categories and let us use only $k+1$ carefully chosen selection procedures.  Now suppose the link defines any invertible deterministic function between $X$ and $Y$.  In this case, with enough enough samples, we can determine both that the relationship is deterministic and the exact specification of the invertible function.  This result is proven in Appendix \ref{sec:casestudies}, Theorem \ref{thm:miracle}.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                 _     _       _        
%  _ __ ___  __ _| | __| | __ _| |_ __ _ 
% | '__/ _ \/ _` | |/ _` |/ _` | __/ _` |
% | | |  __/ (_| | | (_| | (_| | || (_| |
% |_|  \___|\__,_|_|\__,_|\__,_|\__\__,_|
                                       
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Empirical results for cell-types}

\figgy{
\hfill{}\figgygr{.95}{../images/allenNlxy}\hfill{}
\caption{{\bf The input to the Markov link method: one experiment for each measurement procedure and each selection procedure.}  Here we show a portion of the real-world data to which we applied the MLM.  The Allen Institute gathered neurons from the visual cortex of mice, using a variety of cre/lox-based selection procedure (cf.\ \cite{tasic2017shared}).  Each strategy was designed to bring out different subpopulations of cells.  Neurons were measured to determine their `type,' using one of two procedures: Standard or Patch.  Standard recognizes 104 different types of neurons.  Patch has a coarser notion of cell-type, distinguishing only 10 types.  For each experiment, we tabulated the number of cells assigned to each type.  Above we show a subset of these results through heatmaps; the color of each square indicates the number of specimens found to have a particular type.  Using this kind of data, the task is to calibrate the two classification protocols.  That is, we want to be able to ask question of the following form: ``if a neuron is classified as being of type `Peri Kcnj8' by Standard, how might it have been classified by Patch?'' \label{fig:alleninput}}
}

\subsection{Background}

Our motivation for this problem arose from the Allen Institute's investigation of neuronal cell-types.  The institute has a variety of methods for examining a neuron and determining what kind of neuron it is.  However, many of these procedures destroy the neurons in the process of measuring them.  We'll look at two procedures in particular, which we'll call `Standard' and 'Patch.'  Standard uses a single-cell RNA sequencing pipeline to determine transcriptomic expression in the cell and determines the cell-type based on these results.  Patch also uses transcriptomic information from RNA sequencing.  Patch additionally obtains electrophysiological and morphological properties of the neuron.  This comes at the cost of a degraded transcriptomic signal, requiring new methods to estimate the cell-type.  We refer the reader to \citep{tasic2017shared} for details on the two methods.

Best efforts were made to use biological intuition to calibrate the methods.  For example, Patch has a notion of a ``Lamp5'' cell type.  Standard gives a more granular analysis, dividing this type into many sub-types, such as ``Lamp5 Pdlim5'' and ``Lamp5 Slc35d3.''  If a cell was designated as ``Lamp5 Pdlim5'' using Standard, the hope was that it be given the ``Lamp5'' type by Patch.  The two methods were designed to achieve this goal.  However, each method has its own biases and errors, and it was not obvious whether this effort was successful.  In particular, it seemed clear that sometimes a cell labelled one way with one method would get labelled quite differently with another method, but it was not clear how often this occurred.  

Fortunately, there was a kind of information that seemed like it might help determine whether the two methods were properly calibrated: a variety of selection procedures.  Using a cre/lox system (cf.\ \citep{tasic2017shared}) they were able to pick out various overlapping subpopulations of neurons using different procedures.  Each procedure was expected to yield different proportions of the different cell-types.  For each selection procedure and each measurement procedure, many specimens were sampled and their cell-types determined.  The result of this process was two tables, parts of which are shown in Figure \ref{fig:alleninput}.  While it seemed clear that these tables should say something about the calibration, it was not obvious how to best use this information.  It was for this purpose that the MLM was developed.

\subsection{Results}

\figgy{
\hfill{}\figgygr{0.9}{../images/allenout}\hfill{}
\caption{{\bf MLM estimation of the link, with credible interval.}  The central plot shows a portion of the MLM estimator applied to the Allen Institute data.  We look at a subselection of Standard types ($x$) and all of the Patch types ($y$).  For each combination $x,y$ we draw a rectangle whose color indicates the value of the estimator $\hat \pxy(y|x)$.  We also determine the \MLM{}'s confidence about these estimators.  On the left we indicate the lower bounds indicated by the credible interval, on the right we indicate upper bounds.  For some aspects of the link the intervals are much tighter than others.  For example, it appears we have high confidence the Standard type `Vip Rspo4' is highly associated with the Patch type `Vip.'  By constrast, we have almost no idea what is associated with the Standard type `Meis2.'  The upper credible interval bounds suggest that $\pxy(y|\mathrm{Meis2})$ could be nearly 1 for many different Patch types.  Obviously it cannot be 1 for all of those types simultaneously, since $\sum_y \pxy(y|\mathrm{Meis2})=1$, but the data simply doesn't tell us which $y$ carries the mass.  \label{fig:allenboot}}
}

We applied the MLM to this data to obtain point estimates $\hat \pxy(y|x)$ and credible intervals $C_{x y}$.  In Figure \ref{fig:allenboot} we visualize these objects for selected values of $x,y$.  Each part of the link has a slightly different story.  For example, for the Standard type $x=$`Vip Rspo4' and the Patch type $y=$`Vip,' we have that $C_{x,y}=[.88,1.0]$.  This supports the idea that the true link satisfies $\pxy(y|x)\geq .88$: at least 88\% of the cells classified as type `Vip Rspo4' by the Standard method will be classified as `Vip' by the Patch method.  However, for other types there is more ambiguity.  For the Standard type $x=$`Lamp5 Egln 1' and the Patch types $y=$`Lamp' we have $C_{x y}=[0,1.0]$.  The data we have does not give us a definitive answer as to whether cells with Standard type `Lamp5 Egln 1' are being classified with Patch type `Lamp.'

The variability in the credible regions suggests what we need to do in order to more closely determine the value of the calibration.  For example, if we could develop more unique sampling strategies which will include `Lamp5 Egln 1' cells, this would help us resolve the ambiguity about this aspect of the link.  Indeed, going back to the original data, it is easy to see why this ambiguity appeared in the first place.  Cells of the `Lamp5 Egln 1' type only appear in any number when using the sampling strategies `Gad2-IRES-Cre' and 'Slc32a1-IRES-Cre.'  Both of those sampling strategies yield a fairly similar mix of types when measured with Patch.  To get a better resolution of the calibration, we would need a sampling strategy that included `Lamp5 Egln 1' cells but represents a significantly different slice of the overall population.  For a particular proposed experiment, simulations such as those found in Section \ref{sec:simulations} can be used to determine how many samples might be required to get an accurate estimate of the link, $\pxy$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%            _                               _    
%  _ __  _ __(_) ___  _ ____      _____  _ __| | __
% | '_ \| '__| |/ _ \| '__\ \ /\ / / _ \| '__| |/ /
% | |_) | |  | | (_) | |   \ V  V / (_) | |  |   < 
% | .__/|_|  |_|\___/|_|    \_/\_/ \___/|_|  |_|\_\
% |_|                                              
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Relation to prior work}

The task of this paper was infer the link between different measurement procedures, so that experimental data can be meaningfully combined.  There is an enormous literature on this subject.  For example, when experiments are performed in batches, the exact measurement procedures can vary slightly between batches.  The entire field of `batch effects' is devoted to handling these problems.  The general approach is to use some knowledge of the procedures to make modeling assumptions about the links.  These assumptions give us a way to estimate the link (cf.\ \cite{johnson2007adjusting}).  If different measurement procedures yield results in the same space, we can also implicitly articulate these kinds of assumptions by placing a metric on the space.  We suppose that different measurement procedures should yield results that are `close' in this metric.  We can then use optimal transport techniques to produce a link based on these assumptions (cf.\ \cite{tabak2018explanation}).  From the most general point of view, we are engaged in meta-analysis; we refer the reader to \cite{borenstein2011introduction} for a general introduction to the field.  

The main distinguishing characteristics of this paper are two-fold: we take the identifiability issues seriously and place no assumptions on the nature of the link.  We refer the reader to \cite{walter2014identifiability} for an introduction to what we mean by identifiability.  We discuss the identifiability issues for this problem in particular in Appendix \ref{sec:casestudies}.  As for assumptions, the only assumption is that the selection procedures can be statistically isolated from the link.  This assumption says nothing about what the link itself is.  We take a Bayesian approach, but we only use prior beliefs to estimate quantities which actually can be estimated, carefully avoiding the identifiability issues.  We use these quantities to produce intervals which account for uncertainty due both to small sample sizes and to identifiability issues.

The main technical contribution of this paper was figuring out how to use the MLM assumption to get credible intervals that worked in practice.  In this we were inspired by a large literature of examples where assumptions are used to bound potentially unidentifiable parameters.  Some of this literature comes from the field of causality.  For example, in \cite{bonet2001instrumentality} Bonet produces regions not unlike the ones seen here to explore whether a variable can be used as an instrument.  The famous Clauser-Horne-Shimony-Holt inequality was designed to help answer causality questions in quantum physics, but it also sheds light on what distributions are consistent with certain assumptions \cite{clauser1969proposed}.  More generally, the physics literature has contributed many key assumptions that bound unidentifiable parameters (cf. \cite{chaves2014inferring}, \cite{kela2017semidefinite}, and the references therein).  Perhaps the closest work to this one would be \cite{makarov1982estimates}, which used two marginal distributions to get bounds on a property of the joint distribution (namely the distribution of the sum).  We advance this approach to a more general-purpose technique, both by using many subpopulations to closely refine the \MLM{} estimates and by considering the entire space of possible joint distributions instead of simply a particular property of the joint.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                       _           _                 
%   ___ ___  _ __   ___| |_   _ ___(_) ___  _ __  ___ 
%  / __/ _ \| '_ \ / __| | | | / __| |/ _ \| '_ \/ __|
% | (_| (_) | | | | (__| | |_| \__ \ | (_) | | | \__ \
%  \___\___/|_| |_|\___|_|\__,_|___/_|\___/|_| |_|___/
%                                                    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Discussion}

It can be difficult to understand how two different measurement procedures are related to each other.  This makes it difficult to understand how to combine knowledge from experiments which use different procedures.  The problem is particularly tricky if we cannot look at the same specimen through the perpsective of both procedures simultaneously.

In this paper we suggest one way to overcome these difficulties.  We formalize the `relationship' between two measurement procedures through the notion of a `measurement link.'  Given that we have measured a specimen with one procedure and obtained the outcome $x$, the link $q(y|x)$ tells us the probability that we would obtain outcome $y$ if we measured the same specimen with the second procedure.  We present the Markov link method assumption, which roughly states that the link can be statistically isolated from the selection procedures.  We provide the Markov link method (MLM) algorithm that uses this assumption to estimate the link and measure uncertainty.

The MLM appears gives provable guarantees, provides reasonable values in simulation, and gives useful insight to real data.  Applied to neuronal cell-type data, the MLM clarified how two different cell-type classification systems are related.  We saw that some aspects of the two systems seem well-calibrated, but others we are less sure about.  The nature of the variability in the credible intervals suggested directions for experiments which would further refine our understanding.  

In future work, it would be interesting to use additional assumptions to help us estimate calibrations.  For example, as it stands the Markov link method will only yield narrow credible intervals if each measurement tool returns one of a finite number of measurement outcomes.  In some cases, we may believe that similar measurement values should have similar probabilities.  Such smoothness assumptions would make it possible to apply the MLM to measurement tools which can return a continuum of values.   

Another direction for future work would be to take a frequentist point of view.  An obvious direction would be to use profile likelihoods as statistics to define confidence intervals for the link.  However, the distribution of these statistic is difficult to pin down.  Traditional techniques based on asymptotic normality may fail dramatically, because the true link may lie at the boundary of the parameter space.  In particular, there may be some $x,y$ for which $\pxy(y|x)=0$.  Even approximate methods are difficult to apply because the parameter space can be quite high-dimensional.  Perhaps future work could uncover a way to overcome these challenges.

It is clear that good assumptions can help us get real insight for tough problems. Even if these assumptions are not sufficient to allow us to perfectly identify objects of interest, we can still get uncertainty bounds.  By probing this uncertainty carefully, we can learn what the data actually has to say and what experiments will help us learn more.

\bibliographystyle{unsrt}
\bibliography{refs}

\appendix


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                   __ _     _                     
%   ___ ___  _ __  / _(_) __| | ___ _ __   ___ ___ 
%  / __/ _ \| '_ \| |_| |/ _` |/ _ \ '_ \ / __/ _ \
% | (_| (_) | | | |  _| | (_| |  __/ | | | (_|  __/
%  \___\___/|_| |_|_| |_|\__,_|\___|_| |_|\___\___|
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exact details of the Markov link method}

\label{sec:mlmdetails}
 
Consider experiments yielding an $\Omega_\ell\times\Omega_X$ matrix $\DX$ and an $\Omega_\ell \times \Omega_Y$ matrix $\DY$, carrying the distribution
\begin{align*}
(\DX_{\ell 1},\DX_{\ell 2}\cdots \DX_{\ell \Omega_X}) \sim \mathrm{Multinomial}(n_\ell,\plx(\cdot | \ell))\\
(\DY_{\ell 1},\DY_{\ell 2}\cdots \DY_{\ell \Omega_Y}) \sim \mathrm{Multinomial}(m_\ell,\ply(\cdot | \ell))
\end{align*}
where $\plx(x|\ell),\ply(y|\ell)$ are conditional distributions and there is some $\pxy(y|x)$ such that $\ply(y|\ell)=\sum_x \plx(x|\ell)\pxy(y|x)$.  This is simply a restatement of the assumptions we have made throughout this paper about how the objects of interest $(\plx,\pxy,\ply)$ are related to the data we can observe $(\DX,\DY)$.

The purpose of the MLM is to make estimates about $\pxy$ using the data $\DX,\DY$.  Unfortunately, $\pxy$ cannot be directly determined from the data.  Even perfect knowledge of $\plx,\ply$ may be insufficient to determine the true value of $\pxy$.  Some examples are detailed in Appendix \ref{sec:casestudies}.  This problem is called `nonidentifiability,' and it can have some troubling consequences.  For example, standard Bayesian analyses applied to nonidentifiable parameters will be extremely sensitive to the precise choice of prior beliefs.  Even with infinite data, the prior beliefs may have a significant impact on inferences.  To avoid these difficulties, we focus on objects that we know we can identify from data.  In particular, we will look at lower bounds, upper bounds, and something in-between.

Let $\Theta(\plx,\ply)=\{\pxy:\ \ply(y|\ell) = \sum_x \plx(x|\ell)\pxy(y|x)\}$ denote the set of links which are consistent with $\plx,\ply$ and the Markov link method assumption.  We define
\begin{itemize}
    \item $\pxy_{\mathrm{lo},\plx,\ply}(y|x) \triangleq \min_{\pxy\in \Theta(\plx,\ply)} \pxy(y|x)$
    \item $\pxy_{\mathrm{hi},\plx,\ply}(y|x)\triangleq \max_{\pxy\in \Theta(\plx,\ply)} \pxy(y|x)$ 
    \item $\hat \pxy_{\plx,\ply} \triangleq \argmin_{\pxy\in \Theta(\plx,\ply)} D_{f}(\mathrm{Uniform}||\pxy)$, where $D_f$ is some $f$-divergence.\footnote{In practice, we choose a $\chi^2$ divergence because it makes the minimization problem a quadratic program; this makes it particularly easy to solve.  See Appendix \ref{sec:optproblems} for details.}
\end{itemize}
Even if $\pxy$ is nonidentifiable, we can still be assured that $\pxy_{\mathrm{lo}}, \pxy_{\mathrm{hi}}$ are identifiable and $q_{\mathrm{lo}}(y|x) \leq \pxy(y|x) \leq \pxy_{\mathrm{hi}}(y|x)$.  The estimator $\hat \pxy$ is also identifiable and we can also hope it will strike a middle ground.  In producing this single point estimate we had to decide how to deal with the fundamental fact that actually any $\pxy \in \Theta$ might be correct.  At a basic level, we could make two kinds of mistakes.  We might claim a very strong association between the measurement procedures even though actually there is none.  We might claim a very weak association even though actually there is a strong association.  We choose to err on the side of asserting weak associations, by choosing the $\pxy$ which is as close as possible to uniform.  We made this choice in the spirit of the Maximum Entropy Principle, i.e.\ that in the absence of other information we assume $X$ is associated with each $Y$ equally.  This is perhaps as reasonable as any way to pick a particular $\hat \pxy$.  However, we reiterate that $\hat \pxy$ is just one possibility among many.  It is safest to consider the full spectrum of possibilities by looking at the extremes $\pxy_{\mathrm{lo}}, \pxy_{\mathrm{hi}}$.

If we had perfect knowledge of $\plx,\ply$, the objects $\pxy_{\mathrm{lo},\plx,\ply}, \pxy_{\mathrm{hi},\plx,\ply},\hat \pxy_{\plx,\ply}$ would give us a reasonable understanding of what we can know about the link $\pxy$.  However, in practice we do not have access to $\plx,\ply$.  Instead, we have access to the data $\DX,\DY$ which enables us to estimate $\plx,\ply$.  To account for uncertainty about these estimates, we take a Bayesian perspective.  For prior beliefs about $\plx,\ply$, we take a noninformative uniform prior $\mathbb{P}$:
\[
\mathbb{P}(\plx,\ply) \propto 1
\]
Following the Bayesian philosophy, we then incorporate new knowledge by conditioning.  We have two important pieces of knowledge about $\plx,\ply$.  First, we have observed the data, $\DX,\DY$.  Second, we know from the MLM assumption that there exists \emph{some} value $\pxy$ such that Equation (\ref{eq:mlmh}) holds.  We would like to condition on both of these facts.  However, due to the Borel-Kolmogorov paradox, `conditioning on the MLM assumption' is not a meaningful idea.  Instead, it is necessary to define a variable indicating how much the MLM assumption fails, and condition on this variable being zero.  In particular, let $D(h||h')$ denote the Kullback-Leibler divergence and $\Gamma(\plx,\ply) = \min_{\ply':\ \Theta(\ply,\ply')\neq \emptyset} D(\ply||\ply')$.  Posterior uncertainty about $\plx,\ply$ can then be articulated through the distribution
\[
\mathbb{P}\left(\plx,\ply | \DX,\DY,\Gamma(\plx,\ply)=0\right)
\]
In terms of this posterior, we define the \MLM{} point estimate $\hat \pxy$ and uncertainty bounds $C$ as follows:
\begin{enumerate}
    \item $\hat \pxy$ is calculated using posterior expectation:
    \[
    \hat \pxy \triangleq \mathbb{E}\left[\hat \pxy_{\plx,\ply} | \DX,\DY,\Gamma(\plx,\ply)=0\right]
    \]
    %
    \item $C_{x,y}$ is calculated as credible intervals.  For each $x,y$, we define $C_{x,y}$ to be the largest interval such that 
    \begin{align*}
    \mathbb{P}\left(\pxy_{\mathrm{lo},\plx,\ply}(y|x) \in C_{x,y} | \DX,\DY,\Gamma(\plx,\ply)=0\right)\geq 97.5\% \\
    \mathbb{P}\left(\pxy_{\mathrm{hi},\plx,\ply}(y|x) \in C_{x,y} | \DX,\DY,\Gamma(\plx,\ply)=0\right)\geq 97.5\% 
    \end{align*}
\end{enumerate}

In practice, we were not able to find a way to compute these objects exactly.  Given samples from the posterior distribution $\mathbb{P}(\plx,\ply| \DX,\DY,\Gamma(\plx,\ply)=0)$, it would be straightforward to get good estimates.  As seen in Appendix \ref{sec:optproblems}, it is straightforward to compute $\pxy_{\mathrm{lo}},\pxy_{\mathrm{hi}},\hat \pxy$ from samples of $\plx,\ply$, so we could use use monte-carlo approximations for our objects of interest.  Unfortunately, it seems difficult to obtain samples from this posterior distribution.   Common approaches to this type of problem involve Markov-Chain Monte Carlo and Variational Inference, but we were unable to make these approaches work in practice.  It seems nontrivial to work with the condition $\Gamma(p,h)=0$ that formalizes the \MLM{} assumption.  We instead take a somewhat na\"ive approach.  We start by drawing samples according to
\[
F,H ~ \sim \mathbb{P}(\plx,\ply | \DX,\DY)
\]
This can be achieved exactly, using the the conjugacy between the prior and the Multinomial distribution.  Notice that these samples do not incorporate knowledge of the \MLM{} assumption, insofar as they are not conditioned on the event $\Gamma(\plx,\ply)=0$.  To approximately remedy this, we define $\tilde H$ as the solution of $\min_{h'}D(H|h')$, subject to the constraint that $\Gamma(F,h')=0$.  Optimization details can be found in Appendix \ref{sec:optproblems}.  We use the pair $F,\tilde H$ as \emph{approximate} samples for the distribution $\mathbb{P}(\plx,\ply| \DX,\DY,\Gamma(\plx,\ply)=0)$.  We can repeat this process to produce many samples of $(F,\tilde H)$ and use those samples to produce approximate monte-carlo estimates for $\hat \pxy(y|x),C_{x,y}$.  We asymptotically expect that $F,H$ will nearly satisfy the MLM assumption in any case, so this approximation should not make a large difference.  For example, on the Allen Institute data we found that the total variation distance between $H(\cdot|\ell)$ and $\tilde H(\cdot|\ell)$ was about 15\% (averaging over all selection procedures $\ell$ and various samples of $H$).  For comparison, this is about three times smaller than the average total variation distance between $H(\cdot|\ell)$ and $H(\cdot|\ell')$ for a randomly selected pair of selection procedures $(\ell,\ell')$, which averages out to around 50\%.   In any case, it is a practical solution to a difficult problem.

\section{Numerical issues}

\label{sec:optproblems}

There are three numerical problems which the MLM must solve.  Here we detail the \MLM{} method for solving each of them.

\begin{enumerate}

    \item Projecting to the MLM assumption.  Fix any values for $F,H$.  One step in the MLM involves projecting $H$ to the set of distributions which are consistent with $F$ and the MLM assumption.  In particular, we defined
    \[
    D(h|h') = \sum_{\ell,y} h(y|\ell) \log \frac{h(y|\ell)}{h'(y|\ell)}
    \]
    and we needed to solve the problem
    \[
    \min_{h} D(H|h)
    \]
    subject to the constraint that there exists some $q$ such that $h(y|\ell)=\sum_x F(x|\ell)q(y|x)$.  Parametrizing valid $h$ through $\pxy$, we obtain the problem
    \[
    \max_{\pxy} \sum_{\ell,y} H(y|\ell) \log \left(\sum_x F(x|\ell) \pxy(y|x)\right)
    \]
    Taking derivatives one can readily show that this problem is convex.  We solve it using exponentiated gradient ascent (cf.\ \cite{kivinen1995additive}).  We initially guess that $\pxy$ is uniform.  We then repeatedly make the updates
    \begin{align*}
    \pxy(y|x)  &\propto \pxy(y|x)\sum_{\ell}F(x|\ell)\frac{H(y|\ell)}{\sum_x F(x|\ell)\pxy(y|x)}\\
    \end{align*}
    until convergence.  The algorithm's convergence criteria is that all parameters change less than $10^{-5}$ in a single iteration.

    \item Linear programming.  Fix any $\plx,\ply$.  To deal with the identifiability issues, we defined $\Theta(\plx,\ply)=\{\pxy:\ \ply(y|\ell) \triangleq \sum_x \plx(x|\ell)\pxy(y|x)\}$.  The MLM requires us to solve linear optimization problems within $\Theta$, such as 
    \[
    \min_{\pxy\in \Theta(\plx,\ply)} \pxy(y|x) \\
    \]
    We solve these problems using the {\tt cvxopt} python package.

    \item Quadratic programming.  To obtain the minimum $\chi^2$ divergence to uniform, the MLM also requires us to solve quadratic optimization problems within $\Theta$:
    \[
    \min_{\pxy \in \Theta(\plx,\ply)} \sum \pxy(y|x)^2 \\
    \]
    We solve these problems using the {\tt cvxopt} python package.


\end{enumerate}

\section{Identifiability}

\label{sec:casestudies}

The issue of identifiability comes up repeatedly throughout this paper.  Here we give a brief overview of the fundamentals of this issue and how it effects us.  We also present two suggestive case studies which we hope may inspire future research.  In both cases we are able to prove something of interest -- but not quite as much as we might hope.  Here we will use the notation introduced in Appendix \ref{sec:mlmdetails}.

First note that we can obtain arbitrarily good estimates of $\plx,\ply$ by taking enough samples (i.e.\ taking $n_\ell,m_\ell$ sufficiently high).  Let us therefore imagine for a moment that we in fact have \emph{perfect knowledge} of $\plx,\ply$.  Even so, the data does not necessarily tell us the value of the link $\pxy$.  There may be many possible links, $\pxy$, which are all equally consistent with $\plx,\ply$.  That is, we may have $\pxy_1,\pxy_2$ such that $\ply(y|\ell)=\sum_x \plx(x|\ell)\pxy_1(y|x)=\ply(y|\ell)=\sum_x \plx(x|\ell)\pxy_2(y|x)$.  Both links yield the exact same distribution on the data we can observe, so there can be no way to use data to distinguish among them.  This is known as a `nonidentifiability problem.'  Even with infinite data, we simply cannot identify exactly what the value of $\pxy$ might be.

We will now look at some examples:

\subsection{A simple failure case}

Consider the case that $\Omega_\ell = \Omega_Y = 2$ and $\Omega_X=3$.  That is, there are $2$ separate selection procedures, tool I recognizes 3 categories and tool II recognizes 2 categories.  In particular, let us imagine that $\plx(x|\ell)=A_{\ell x}$ and $\ply(y|\ell)=B_{\ell y}$ where $A,B$ are matrices given by 
%
\begin{align*}
A&=\left(\begin{array}{ccc}
\frac{1}{2} & \frac{1}{2} & 0\\
\frac{2}{6} & \frac{1}{6} & \frac{3}{2}
\end{array}\right) \\
B&=\left(\begin{array}{cc}
\frac{1}{2} & \frac{1}{2}\\
\frac{2}{3} & \frac{1}{3}
\end{array}\right)
\end{align*}

Rows correspond to different selection procedures and columns correspond to a different measurement outcome.  Now let $\pxy(y|x)=C_{x y}$, another matrix.  The Markov link method assumption then tells us that $A\times C=B$, where $\times$ indicates matrix multiplication.  This corresponds to $\Omega_\ell \times \Omega_Y = 4$ equations.  We also have a normalizing constraint that $\sum y \pxy(y|x)=1$, which creates $\Omega_X=3$ additional equations.  However, these normalizing constraints actually make two of the MLM assumption constraints redundant.  In the end, we have $5$ constraining equations on the matrix $C$.  However, the matrix $C$ contains six numbers.  The result is a degree of freedom in $C$, corresponding to an aspect of $\pxy$ that we simply cannot resolve.  For example, here are two choices of $C$ which are both consistent with the equation $A\times C=B$:
\[
C=\left(\begin{array}{cc}
0 & 1\\
1 & 0\\
1 & 0
\end{array}\right)\qquad C=\left(\begin{array}{cc}
1 & 0\\
0 & 1\\
\frac{2}{3} & \frac{1}{3}
\end{array}\right)
\]



\subsection{Permutation matrices}

Consider the case that $\Omega_\ell = k$ and $\Omega_X=\Omega_Y=2^{k-1}$.  That is, there are $k$ separate subpopulations, and both tool I and tool II can return one of $2^{k-1}$ possible values.  Let us furthermore assume that
\[
\pxy(y|x)=\begin{cases}1 \quad \mathrm{if}\ x=y\\0\quad \mathrm{else}\end{cases}
\]
and  $\plx(x|\ell)=A_{\ell,x}$, where this matrix is given by
\[
A=2^{2-k}\left(\begin{array}{ccccccccccc}
0 & 1 & 0 & 1 & 0 & 1 & \cdots & 0 & 1 & 0 & 1\\
0 & 0 & 1 & 1 & 0 & 0 & \cdots & 0 & 0 & 1 & 1\\
0 & 0 & 0 & 0 & 1 & 1 & \cdots & 1 & 1 & 1 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 1 & 1 & 1\\
 &  &  &  &  &  & \vdots\\
0 & 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 1 & 1 & 1\\
1 & 0 & 1 & 0 & 1 & 0 & \cdots & 1 & 0 & 1 & 0
\end{array}\right)
\]
That is, the $x$th column of the first $k-1$ rows is the binary expansion of the number $x-1$, and the last row alternates $1$s and $0$s.  Now let us say we have perfect knowledge of $\plx(x|\ell)$ and $\ply(y|\ell)=\sum_x \plx(y|\ell)\pxy(y|x)$.  Notice that due to the simple structure of $\pxy$ we obtain $\ply(y|\ell)=A^{\ell,y}$.  However, let us imagine we know nothing about the true value of $\pxy$.  

How much can we say about $\pxy$, if we only had knowledge of $\plx$ and $\ply$?  On the one hand, we observe that in the absence of any other constraints, the object $\pxy$ has $2^{2k-3}$ degrees of freedom.  This is because there are $2^{k-1}$ values of $\ell$ and for each subpopulation $\pxy(\cdot|\ell)$ must lie in the $2^{k-2}$-dimensional simplex on $2^{k-1}$ atoms.  On the other hand, we see that the Markov Link Assumption gives us $k\times(2^{k-1}-1)$ linear constraints on the value of $q$.  Indeed, for each subpopulation in $1\cdots k$ and each value of $y \in 1 \cdots 2^{k-1}$ we have an equation of the form
\[
\sum_x p(y|\ell)q(y|x)=h(y|\ell)
\]
Of these $k\times2^{k-1}$ constraints, $k$ of them are redundant with the fact that $\sum_y \pxy(y|x)=1$.  Thus, altogether, the Markov Link Assumption together with approximate knowledge of $p,h$ gives us $k\times(2^{k-1}-1)$ linear constraints.  It would follow that $q$ would have $2^{2k-3} - k\times(2^{k-1}-1)$ degrees of freedom yet remaining.  

In conclusion, a simple degrees-of-freedom counting argument would suggest that there will be substantial ambiguity about what value $q$ might take on, if our only knowledge about $q$ is that it must satisfy $\sum_x \plx(y|\ell)q(y|x)=\ply(y|\ell)$.  Indeed, we have \emph{exponentially many} more degrees of freedom than we have constraints.  

However, the reality is that $q$ is exactly determined by $\plx,\ply$.  This is possible because there are inequality constraints which also govern $q$, namely $\pxy(y|x)\geq 0$.  Thus, while a simple degrees-of-freedom counting argument might suggest that we would have substantial identifiability issues in this problem, the reality is quite the opposite.  This idea is made rigorous in the following theorem.  

\begin{thm}\label{thm:miracle}
Let $\plx,\ply$ be as they are defined above.  Then there is exactly one $\pxy$ that is consistent with $\plx,\ply$ and the Markov Link assumption.  That is, $\pxy$ is the only possible value satisfying
\begin{gather*}
\sum_y \pxy(y|x)=1 \\
\sum_x A_{\ell,x}\pxy(y|x)=A_{\ell,y}\\
q(y|x)\geq 0
\end{gather*}
\end{thm} 
\begin{proof}
We prove by recursion.  First take the case $k=2$.  In this case the result holds trivially, since $X,Y\in\{1\}$.

Now consider a general case $k>2$.  Let us focus on the constraints implied by the second-to-last row population.  It is straightforward to see that these constraints imply
\begin{align*}
0=&\pxy(y|x) \qquad \forall y\leq 2^{k-2},x>2^{k-2}
\end{align*}
Indeed, for each $y\leq 2^{k-2}$ we obtain a constraint showing that $\sum_{x>2^{k-2}}q(y|x)=0$, which yields that in fact $\pxy(y|x)=0$ for every $x>2^{k-2}$ and every $y\leq 2^{k-2}$.

It follows that for $y\leq 2^{k-2}$ the original constraints may be rewritten as
\[
\sum_{x\leq2^{k-2}} A_{\ell x} \pxy(x|y) = A_{\ell y} \qquad \forall y\leq 2^{k-2}
\]
This is an example of the same problem we started with -- except with $k$ one smaller.  Applying the inductive hypothesis, we may thus obtain that $\pxy(y|x)$ is uniquely determined for the first $2^{k-2}$ values of $x,y$.  Moreover, since $\sum_{y\leq 2^{k-2}} \pxy(y|x)=1$, we see that $\pxy$ must also satisfy $\pxy(y|x)=0$ for $y>2^{k-2}$ and $x\leq 2^{k-2}$.  Thus we have seen that $\pxy$ is uniquely identified for all entries except those in which $x,y\geq 2^{k-2}$.

For $x,y \geq2^{k-2}$ we linearly combine equations concerning the first, last, and second to last rows of $A$ with factors of $1,1,-1$ respectively.  We obtain constraints showing that $\sum_{x\leq2^{k-2}}\pxy(y|x)=0$ for each $y>2^{k-2}$.  We can then use the same reasoning to obtain that $\pxy$ is uniquely identified for the remaining values of $x,y$.
\end{proof}

This result is somewhat robust to slight perturbations in $\plx,\ply$.  In particular, if we have some $\hat \plx\approx \plx$ and $\hat \ply\approx \ply$ then at each stage of the argument we can replace statements of the form $\pxy(y|x)=0$ with statements of the form $\pxy(y|x)\leq \epsilon$.  Applying this with the kinds of arguments above will show that we can be sure that every point in $\Theta(\hat \plx,\hat \ply)$ is arbitrarily close to $\pxy$ if we know that $\hat \plx,\hat \ply$ are sufficiently close to $\plx,\ply$.  

However, it turns out that the relationship between $\pxy$ and $\plx,\ply$ is not robust in every situation.  In the next section we will see that it can in fact be quite discontinuous:

\subsection{Discontinuity}

Consider the case that $\Omega_\ell = 1$ and $\Omega_X=\Omega_Y=2$.  That is, there is only one selection procedure (no subpopulations) and both tool I and tool II can return one of $2$ possible values.  We will now consider two possiblities:
\begin{enumerate}
\item First let us take the case
    \begin{itemize}
    \item $\mathbb{P}(X=1)=\plx(1)=0$
    \item $\mathbb{P}(X=2)=\plx(2)=1$
    \item $\mathbb{P}(Y=1)=\ply(1)=0$
    \item $\mathbb{P}(Y=2)=\ply(2)=1$
    \end{itemize}
    In this case the MLM assumption $\sum_x \plx(x)\pxy(y|x)=\ply(y)$ can be used to prove that $\pxy(1|2)=0,\pxy(2|2)=1$, but we now have \emph{absolutely no} knowledge of $\pxy(1|1),\pxy(2|1)$.  This is because we simply never observed the case $X=1$ (it occurs with probability zero), and so we cannot possibly have any knowledge about $\pxy(y|x)$ for $x=1$.  
\item Now let us take a slight variation:
    \begin{itemize}
    \item $\mathbb{P}(X=1)=\plx(1)=0.01$
    \item $\mathbb{P}(X=2)=\plx(2)=0.99$
    \item $\mathbb{P}(Y=1)=\ply(1)=0$
    \item $\mathbb{P}(Y=2)=\ply(2)=1$
    \end{itemize}
    In this case we can again prove that $\pxy(1|2)=0,\pxy(2|2)=1$, but we can also prove that $\pxy(1|1)=0,\pxy(2|1)=1$.  
\item Now we take yet another slight variation:
    \begin{itemize}
    \item $\mathbb{P}(X=1)=\plx(1)=0.01$
    \item $\mathbb{P}(X=2)=\plx(2)=0.99$
    \item $\mathbb{P}(Y=1)=\ply(1)=0.01$
    \item $\mathbb{P}(Y=2)=\ply(2)=0.99$
    \end{itemize}
    In this case we can prove that $\pxy(1|2)\leq 1/99$ and $\pxy(2|1)\geq 1-1/99$, but we again cannot prove almost anything about $\pxy(2|1)$.  In particular, it is easy to produce cases in which $\pxy(2|1)=0$ and other cases in which $\pxy(2|1)=1$.  

\end{enumerate}

The disturbing thing about this example is that by making infinitesimal perturbations to $\plx$ we can pass from uncertainty to complete certainty back to uncertainty.  It is for this reason that in this paper we refuse to ever treat $\plx,\ply$ as fixed and given, always considering the space of perturbations around any such values.  

It is worth noting that these kinds of problems essentially vanish if the true $\pxy$ is bounded away from zero i.e.\ $\pxy(y|x)>c$ for every $x,y$ for some $c>0$.  If this holds, together with a certain linear independence assumption, we can guarantee that the true $\pxy$ is close to the set $\Theta(\hat \plx,\hat \ply)$ when $\hat \plx,\hat \ply$ are good approximations to $\plx,\ply$, where
\[
\Theta(\plx,\ply) \triangleq \left\{\pxy\in T:\ \sum_x \plx(x|\ell)\pxy(y|x)= \ply(y|\ell) \quad \forall \ell,y \right\}
\]
and by $T$ we mean the transition matrix polytope, $T=\{\pxy:\ \pxy(y|x)\geq 0, \sum_y \pxy(y|x)=1\}$.  In particular:\vspace{.1in}

\begin{thm}\label{thm:mainthm}
Fix any $\pxy^*$ satisfying $\pxy^*(y|x)>c$ for some $c>0$.  Let us further assume that the matrix $B^*_{\ell,x}=\plx^*(x|\ell)$ has linearly independent rows.  Fix any $\plx$ and define $\ply(y|\ell)=\sum_x \plx(x|\ell)\pxy(y|x)$.  Then by taking any $\hat \plx,\hat \ply$ sufficiently close to $\plx,\ply$ we can ensure that $\pxy^*$ is arbitrarily close to some point in the set $\hat \Theta = \Theta(\hat \plx,\hat \ply)$.
\end{thm}
\begin{proof}
Let $\hat A$ denote the affine plane $\hat A=\{\pxy:\ \sum_x \hat \plx(x|\ell)\pxy(y|x)= \hat \ply(y|\ell)\}$.  Thus $\hat \Theta = T \cap \hat A$.

Now fix any $\hat p,\hat h$.  Now let $\tilde \pxy$ be the orthogonal Euclidean projection of $\pxy^*$ to the affine space $\hat A$.  That is, $\tilde q$ is minimizes a sum-of-squares difference to $\pxy^*$ among all the points in $\hat A$.  The linearly independent rows of $B^*$ allow us to bound the spectral norm of the right-pseudoinverse of $\hat B_{\ell,x}=\hat \plx(x|\ell)$, by taking $\hat \plx$ sufficiently close to $\plx^*$.  If we furthermore require $\hat \ply$ sufficiently close to $\ply^*$, we can use this to ensure the projection distance is small.  That is, we can force $\tilde \pxy$ to be arbitrarily close to $\pxy^*$.  Using the fact that $\pxy^*(y|x)>c$ we can thereby furthermore insure that $\tilde \pxy(y|x)\geq0$.  Finally, it is straightforward to see that the projection leaves the constraint $\sum_y \pxy(y|x)=1$ unchanged.  Thus $\tilde \pxy\in \hat \Theta$ and $\tilde \pxy$ is arbitrarily close to $\pxy^*$. 
\end{proof}

Since it is easy to find consistent estimators for $\plx,\ply$, this theorem suggests we can use those estimators to get good estimates for uncertainty about $\pxy$.  In particular, subject to the conditions of the theorem, we have that the bounds of $C_{x,y}$ converge to the bounds of what is identifiable about $\pxy$.  It is certainly a step in the right direction, but we emphasize that the theorem's conditions are nontrivial.  It is our opinion that the linear independence condition is fairly mild (if it is not met then subpopulations can simply be merged together).  However, the positivity condition is quite troubling.  In many cases of interest the true link $\pxy$ has genuine zeros: pairs of measurements between the two tools which are fundamentally incompatible.  In this case such a theorem cannot be applied.  

However, it may be that the above theorem's requirement ($\pxy(x|y)>c>0$) is much stronger than is necessary.  A precise understanding of the discontinuity example above has remained elusive.  Better understanding could lead to more accurate estimates.  We leave it for future work.


\end{document}






































